% \documentclass[review]{elsarticle}

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{url}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage[left=2.5cm,right=2.5cm, top = 2.8cm, bottom = 2.8cm]{geometry}


% \usepackage{draftwatermark}
% \SetWatermarkText{draft}
% \SetWatermarkScale{3}

\begin{document}

\title{Thinning-based representation of INGARCH models, with an intepretation as stochastic epidemic processes}
\author{Johannes Bracher\\
Chair of Statistics and Econometrics, Karlsruhe Institute of Technology, \\ % Bl\"ucherstra{\ss}e 17, 76185 Karlsruhe\\
Computational Statistics Group, Heidelberg Institute for Theoretical Studies}
% \address{Chair of Statistics and Econometrics, Karlsruhe Institute of Technology, \\ % Bl\"ucherstra{\ss}e 17, 76185 Karlsruhe\\
% Computational Statistics Group, Heidelberg Institute for Theoretical Studies}


\newcommand{\juv}{E}

\maketitle


\begin{abstract}
A thinning-based representation of the Poisson and certain compound Poisson (CP) INGARCH models is proposed. This approaches them to the INAR model class, which is equally thinning-based, and allows for the application of branching process theory to derive stochastic properties. Notably, it is straightforward to show that under mild conditions the CP-INGARCH(1, 1) model is geometrically ergodic while all moments of the limiting-stationary distribution are finite. Moreover, the new representation provides an interpretation of INGARCH models as a stochastic epidemic process, specifically a simplified discrete-time SEIR (susceptible-exposed-infectious-removed) model.
\end{abstract}

\bigskip

\begin{center}
\textbf{This is a preprint and has not been subject to peer-review.}
\end{center}

% \begin{keyword}
% branching process, compound distribution, count time series, geometric ergodicity, integer-valued GARCH
% \end{keyword}

\maketitle

\section{Count autoregression based on thinnings and generalized linear regression}
\label{sec:intro}

Two of the most influential approaches for count time series modelling are the INAR (interger-valued autoregressive) and INGARCH (integer-valued generalized autoregressive conditional heteroscedasticity) classes. The INAR class is built on the idea of thinning operations \citep{Steutel1979}, most commonly the binomial thinning operator $\circ$, which is given by
$$
\pi \circ N = \sum_{i = 1}^N Z_i \ \ \ \text{with} \ \ \ Z_i \stackrel{\text{ind}}{\sim} \text{Bin}(1, \pi).
$$
The basic Poisson INAR(1) model $\{X_t, t \in \mathbb{Z}\}$ \citep{McKenzie1985, Al-Osh1987} is defined as
$$
X_t = I_t + \kappa \circ X_{t - 1},
$$
where $0 \leq \kappa \leq 1$ and $\{I_t\}$ is a sequence of independent Poisson random variables with rate $\nu = 0$. This model has been extended in a multitude of ways \cite{Scotto2015}. The INGARCH class \cite{Ferland2006, Fokianos2009}, on the other hand, adopts the idea of generalized linear regression models (GLMs). Its simplest instance, the Poisson INARCH(1) model, is defined as a process $\{X_t, t \in \mathbb{N}_0\}$ with
\begin{align*}
X_t \mid X_{t - 1}, \dots, X_0 & \sim \text{Pois}(\lambda_t)\\
\lambda_t & = \nu + \alpha X_{t - 1},
\end{align*}
$\nu > 0, \alpha \geq 0$, and a fixed starting value $X_0$. The GLM-based perspective, too, has spawned extensive research \citep{Fokianos2016}. It has been noted that the INARCH(1) model can also be written as \citep{Weiss2015}
$$
X_t = I_t + \kappa \star X_{t- 1}
$$
with $I_t \stackrel{\text{ind}}{\sim} \text{Pois}(\nu)$ as in the INAR(1) model and $\star$ denoting Poisson thinning,
$$
\pi \star N = \sum_{i = 1}^N Z_i \ \ \ \text{with} \ \ \ Z_i \stackrel{\text{ind}}{\sim} \text{Pois}(\pi).
$$
Occasionally, also more complex INGARCH models have been represented via thinning operations, see e.g.\ \citep{Lu2021}. Indeed, the original construction of the INGARCH(1, 1) model involved a ``cascade of thinning operations'' \citep[p.927]{Ferland2006}. By and large, however, the two classes have been treated separately. This note introduces a new thinning-based representation of several INGARCH models, thus providing a bridge to the INAR class. Also, the new representation allows for an intuitive interpretation as a stochastic epidemic process, similar to the so-called SEIR (susceptible-exposed-infectious-recovered) model from epidemiology. We start by stating the definitions of various INGARCH models, then provide their thinning-based representations and conclude by exploiting the novel formulation to obtain some stochastic properties.
 


\section{Poisson and compound Poisson INGARCH models}
\label{sec:original_formulation}

The most common extension of the Poisson INARCH(1) model is the Poisson INGARCH(1, 1) model \cite{Ferland2006, Fokianos2009}. It is defined as a process $\{X_t, t \in \mathbb{N}_0\}$ with
\begin{align}
X_t \mid X_{t - 1}, \dots, X_0, \lambda_0 & \sim \text{Pois}(\lambda_t)\label{eq:X_t_original}\\
\lambda_t & = \nu + \alpha X_{t - 1} + \beta \lambda_{t - 1} \label{eq:lambda_t}
\end{align}
where $\nu > 0, \alpha > 0$ and $0 \leq \beta < 1$. Note that this is somewhat more restrictive than most definitions from the literature where $\alpha = 0$ and $\beta \geq 1$ are usually allowed. The initial values $\lambda_0 \geq \nu$ and $X_0 \in \mathbb{N}_0$ are assumed to be fixed as in \cite{Fokianos2009} (note that other authors, like \citep{Ferland2006}, use index set $t \in \mathbb{Z}$ and thus do not require an initialization). More generally, to define the Poisson INGARCH($p$, $q$) model, equation \eqref{eq:lambda_t} is replaced by
\begin{equation}
\lambda_t = \nu + \sum_{i = 1}^p \alpha_i X_{t - i} + \sum_{j = 1}^q \beta_j \lambda_{t - j},\label{eq:lambda_t_pq}
\end{equation}
and additional initial values $\lambda_{1 - q}, \dots, \lambda_0 \geq \nu$ and $X_{1 - p}, \dots, X_0 \in \mathbb{N}_0$ are fixed. We assume $\alpha_i > 0, i = 1, \dots, p$ and $\beta_j \geq 0, j = 1, \dots, q$  with $\sum_{j = 1}^q \beta_j < 1$.

Numerous variations of model \eqref{eq:X_t_original}--\eqref{eq:lambda_t} with conditional distributions other than the Poisson have been suggested (see \cite{Weiss2018} for an overview). A broad class are \textit{compound Poisson} INGARCH models \cite{Goncalves2015, Silva2016}. A random variable $Y$ is said to follow a compound Poisson distribution \cite[Chapter 3]{Feller1968} if it can be written as a randomly stopped sum 
$$
Y = \sum_{i = 1}^N Z_i, \ \ \ Z_1, \dots, Z_N \stackrel{\text{iid}}{\sim} G(\psi)
$$
where $N$ follows a Poisson distribution. Throughout this article we assume that $G(\psi)$, also called the \textit{secondary distribution}, is parameterized by a single parameter $\psi$ and has support $\mathbb{N}_0$. Its mean $\mu_\psi$ and variance $\sigma^2_\psi$ are assumed to be finite. Adapting notation from Wei\ss\ et al \cite[Sec. 2]{Weiss2017}, the CP-INGARCH(1, 1) model is defined as a process $\{X_t, t \in \mathbb{N}_0\}$ with % N_{t - 1}, N_{t - 2}, \dots, N_0,
\begin{align}
N_t \ \mid \ X_{t - 1}, \dots, X_0, \lambda_0 & \sim \text{Pois}(\lambda_t/\mu_\psi) \label{eq:N_CP_original}\\
X_t & = \sum_{i = 1}^{N_t} Z_{i, t} \ \ \text{where} \ \  Z_{i, t} \stackrel{\text{iid}}{\sim} G(\psi)\label{eq:X_CP_original}\\
\lambda_t & = \nu + \alpha X_{t - 1} + \beta \lambda_{t - 1}.\label{eq:lambda_CP_original}
\end{align}
Here, $\lambda_0 \geq \nu$ and $X_0 \in \mathbb{N}_0$ are again fixed. Conditional on the past, $X_t$ follows a compound Poisson distribution with mean $\lambda_t$ under this definition. % Note that in \todo{remove to avoid questions?}  \eqref{eq:N_CP_original}, the condition is also on $N_{t - 1}, \dots, N_1$, but we suppressed this to shorten notation. 
This formulation is somewhat more restrictive than the class discussed in \citep{Goncalves2015} where the parameter $\psi_t$ of the secondary distribution can depend on $\lambda_t$. It nonetheless contains a number of well-known models \cite[Observation 2]{Goncalves2015}:

\begin{itemize}
% \item When choosing $G(\psi)$ such that $\text{Pr}(Z_{i,t} = 1) = 1$ we recover the Poisson INGARCH(1, 1) model.
\item Setting $G(\psi)$ to a logarithmic distribution % $\text{Log}(\psi)$ with mean $\mu_\psi = \psi/\{(1 - \psi)\log(1 - \psi)\}$, equations \eqref{eq:N_CP_original}--\eqref{eq:X_CP_original} imply
% $$
% X_t  \ \mid \ X_{t - 1}, X_{t - 2}, \dots, X_0, \lambda_0 \sim \text{NegBin}(r_t, p),
% $$
% where \todo{(re-check)}
% $$
% r_t = \frac{\lambda_t(1 - \psi)}{\psi}, \ \ p = 1 - \psi.
% $$
% This corresponds to
% leads to a generalization of the negative biomial DINARCH \cite{Xu2012} to an INGARCH(1, 1) model (Example 4.2.1 in \cite{Weiss2018}).
leads to a negative biomial INGARCH(1, 1) model (\cite{Xu2012}; Example 4.2.1 in \cite{Weiss2018}). %, see also Observation 2 in \cite{Goncalves2015} and Example 4.2.1 in \cite{Weiss2018} .
\item The Neyman Type A INGARCH model \cite{Goncalves2015a} results when setting $G(\psi)$ to a Poisson distribution. %$\text{Pois}(\psi)$ distribution. The conditional distribution of $X_t$ given the past is then \todo{re-check parameterization of Neyman Type A}
% $$
% X_t  \ \mid \ X_{t - 1}, X_{t - 2}, \dots, X_0, \lambda_0 \sim \text{NeymanTypeA}(\lambda_t/\psi, \psi).
% $$
% \item Hermite?
\item The generalized Poisson INGARCH(1, 1) model \cite{Zhu2012} is obtained by setting $G(\psi)$ to a Borel distribution. % $\text{Borel}(\psi)$. The conditional distribution of $X_t$ is then given by
% $$
% X_t  \ \mid \ X_{t - 1}, X_{t - 2}, \dots, X_0, \lambda_0 \sim \text{GP}(\lambda_t [1 - \psi], \psi).
% $$
\end{itemize}
% The negative binomial INGARCH model by \cite{Zhu2011} is \textit{not} part of the class as it would require the parameter $\psi_t$ of the secondary distribution to depend on $N_t$ .

\section{Thinning-based representation of INGARCH models}% \todo{Add INGARCH(p, 1). Maybe start with Poisson, then show others as extensions? Would also be easier to understand}
\label{sec:alternative_formulation}

\subsection{The Poisson INGARCH(1, 1) model}
\label{subsec:poisson11}


We first provide an alternative representation of the Poisson INGARCH(1, 1) model \eqref{eq:X_t_original}--\eqref{eq:lambda_t}. It involves exclusively discrete-valued random variables and is given by
\begin{align}
X_t & = I_t + E_t - L_t \label{eq:X_t_thinning_Poisson}\\
E_t & = L_{t - 1} \ + \ \kappa \star X_{t - 1}\label{eq:E_t_thinning_Poisson}\\
L_t & = \beta \circ E_t \label{eq:L_t}
\end{align}
where $\kappa > 0$ and $0 \leq \beta < 1$. Here, $\{I_t\}$ consists of independent Poisson random variables with rate $\tau > 0$ while $\circ$ and $\star$ denote binomial and Poisson thinning as defined in Section \ref{sec:intro}. % , respectively, i.e.\
% \begin{align*}
% \pi \circ N & = \sum_{i = 1}^N Z_i \ \ \ \text{with} \ \ \ Z_i \stackrel{\text{ind}}{\sim} \text{Bin}(1, \pi), \\
% \pi \star N & = \sum_{i = 1}^N Z_i \ \ \ \text{with} \ \ \ Z_i \stackrel{\text{ind}}{\sim} \text{Pois}(\pi).
% \end{align*}
To initialize the process we fix $X_0 \in \mathbb{N}_0, \eta > 0$ and specify $E_0 \sim \text{Pois}\left(\eta \right)$.

It can be shown that $\{X_t\}$ as defined in equation \eqref{eq:X_t_thinning_Poisson}--\eqref{eq:L_t} is a Poisson INGARCH(1, 1) process as given in \eqref{eq:X_t_original}--\eqref{eq:lambda_t}, see Appendix \ref{subsec:derivation_poisson11} for the derivation. The parameter $\beta$ is shared across the two formulations, meaning that the INARCH(1) model corresponds to $\beta = 0$. The remaining parameters of the original formulation can be recovered as
$
\nu = (1 - \beta)\tau$ and $ \ \
\alpha = (1 - \beta)\kappa.
%; \ \
% \lambda_0 = \mu_\psi \left(\tau + \frac{1 - \beta}{\beta} \times \eta\right)
$
Concerning the initialization, equivalence is achieved by setting $\lambda_0 = \tau + (1 - \beta) \times \eta$.

To facilitate verbal description and generalizations in the following sections, we introduce two auxiliary processes,
\begin{align}
A_t & = E_t - L_t \label{eq:A_t}\\
C_t & = \kappa \star X_t \label{eq:C_t},
\end{align}
which allow us to re-write equations \eqref{eq:X_t_thinning_Poisson} and \eqref{eq:E_t_thinning_Poisson} as
\begin{align}
X_t & = I_t + A_t\label{eq:X_t_v2}\\
E_t & = L_{t - 1} + C_{t - 1}.\label{eq:juv_t_v2}
\end{align}

The Poisson INGARCH(1, 1) process \eqref{eq:L_t}--\eqref{eq:juv_t_v2} can be interpreted as a simple model for the spread of an infectious disease. A visualization is provided in Figure \ref{fig:ingarch_flowchart_poisson}.

\begin{figure}
\center
\includegraphics[scale=0.8]{figure/flowchart_ingarch_poisson_SEIR.pdf}
\caption{Interpretation of the Poisson INGARCH(1, 1) formulation \eqref{eq:L_t}--\eqref{eq:juv_t_v2} as a stochastic epidemic process. Solid lines represent results of binomial thinning, dashed lines Poisson thinning or immigration. Note that the two solid arrows starting at $E_t$ do not represent independent binomial thinnings as by construction  $L_t + A_t = E_t$.}
\label{fig:ingarch_flowchart_poisson}
\end{figure}

\begin{enumerate}
\item $X_t$ is the number of individuals who are infectious with a given disease at time $t$. Each of these individuals stays infectious for one time period and \textit{contaminates} a $\text{Pois}(\kappa)$-distributed number $C_t = \kappa \star X_t$ of other individuals, exposing them to the disease.
\item Contaminated individuals enter into an \textit{exposed pool} at the following time point $t + 1$ (exposed meaning that they have contracted the disease, but are not infectious yet). The total number of exposed individuals present in the population at time $t + 1$ is denoted by $\juv_{t + 1}$.
\item At each time $t$, each of the $E_t$ exposed individuals can either remain in the exposed pool (with probability $\beta$) or advance to infectiousness, thus being removed from the pool (with probability $1 - \beta$). The exposed individuals \textit{left} from time $t$ are denoted by $L_t = \beta \circ E_t$ and form part of $E_{t + 1}$. This construction implies that the latent period (time from contamination to infectiousness) is $\text{Geom}(1- \beta)$ distributed (see equation \eqref{eq:geom_distr}).
\item If an exposed individual from $E_t$ \textit{advances} to the infectious stage, it enters into $X_t$. The number of such individuals is denoted by $A_t = E_t - L_t$.
\item At each time $t$, a $\text{Pois}(\tau)$ distributed number $I_t$ of individuals become infectious due to external sources (\textit{imports}).
\end{enumerate}
This is similar to the mechanism of the so-called SEIR (susceptible-exposed-infectious-removed) model commonly used in epidemiology (see e.g. \citep{Britton2019} for a textbook introduction). The INGARCH mechanism is simpler in that it models only the exposed and infectious groups explicitly, but not the susceptible and recovered/immunized. Consequently, it also ignores that any real population would be finite in size. The model will thus not capture the non-linear effects arising in the SEIR model due to the depletion of the susceptible population. For rare diseases in a large population or diseases with short-lived immunity (which can be described by an SEIS model, susceptible-exposed-infectious-susceptible), it may nonetheless be a reasonable approximation; see \citep{Bauer2018} for a related argument linking the SIR (susceptible-infectious-removed) and the INARCH(1) model.

Note that a related model where the Poisson thinning in equations \eqref{eq:E_t_thinning_Poisson}/\eqref{eq:C_t} is replaced by a binomial thinning has been introduced in \cite{Bracher2019}.

\subsection{The Poisson INGARCH($p$, $q$) model}
\label{subsec:poissonpq}

An extension of the thinning-based representation \eqref{eq:L_t}--\eqref{eq:juv_t_v2} to the INGARCH($p$, $q$) case is given by
\begin{align}
X_t & = I_t + A_t \label{eq:Xt_thinning_pq}\\
E_t & = \sum_{j = 1}^q L_{t - j, j} \ \ + \ \ \sum_{i = 1}^p C_{t - i, i}
\end{align}
where
\begin{align}
C_{t, i} & = \kappa_i \star X_t\\
(L_{t, 1}, \dots, L_{t, q}, A_t) \ \mid \ E_t & \sim \text{Mult}\left(E_t; \beta_1, \dots, \beta_q, 1 - \sum_{j = 1}^q \beta_j\right)\label{eq:L_t_mult}
\end{align}
and $\kappa_i, \beta_j > 0, \sum_{j = 1}^q \beta_j < 1$. For the initialization we fix $X_{1 - p}, \dots, X_0$ and set $E_{m} \sim \text{Pois}(\eta_m)$ with $\eta_m > 0$ for $m = 1 - q, \dots, 0$. Again, the parameters $\beta_{1}, \dots, \beta_q$ are the same as in the original formulation \eqref{eq:lambda_t_pq}. The remaining parameters can be obtained as $\nu = \left(1 - \sum_{j = 1}^q\beta_j \right) \times \tau$ and $\alpha_i = \left(1 - \sum_{j = 1}^q\beta_j \right) \times \kappa_i$. For the initialization, one needs to set $\lambda_m = \tau + (1 - \sum_{j = 1}^q\beta_j) \times \eta_m, m = 1 - q, \dots, 0$.

In terms of the interpretation from the previous section the extension has the following implications. A visualization can be found in Figure \ref{fig:ingarch_flowchart_poisson_pq}.
\begin{itemize}
\item $X_t$ now is the number of individuals who became \textit{newly} infectious at time $t$ (in epidemiological terms it describes \textit{incident} rather than \textit{prevalent} cases).
\item The individuals in $X_t$ cannot only expose others to the disease immediately, thus sending them to $E_{t + 1}$, but also in the $p - 1$ subsequent time periods, then sending them to one of $E_{t + 2}, \dots, E_{t + p}$. The number of such exposed individuals entering into $E_{t + i}$ is $C_{t, i} = \kappa_i \star X_t$. The vector $(\kappa_1, \dots, \kappa_q)$ thus describes the infectivity profile over the $p$ time points of contagiousness.
\item In the exposed pool, individuals can ``skip'' time periods and move forward up to $q$ periods in one step. The number of exposed individuals moving from $E_t$ directly to $E_{t + j}$ is denoted by $L_{t, j}$. It is straightforward to show that the latent period then follows a compound geometric distribution (with a categorical distribution with support $\{1, \dots, q\}$ as the secondary distribution) rather than just a geometric. % compounding order $q$
\end{itemize}
Formulation \eqref{eq:L_t_mult} implies $A_t + \sum_{j = 1}^q L_{t, j} = E_t,$ meaning that the $E_t$ exposed individuals can take a variety of paths, but none is lost or added to the system as a whole in step \eqref{eq:L_t_mult}. Note that a similar construction is used in the INAR($p$) model as defined by Alzaid and Al-Osh \cite{Alzaid1990}.

\begin{figure}[h!]
\center
\includegraphics[scale = 0.8]{figure/flowchart_ingarch_poisson_pq_SEIR.pdf}
\caption{Interpretation of a Poisson INGARCH(2, 2) model as a stochastic epidemic process. Solid lines represent results of multinomial thinning, dashed lines Poisson thinning or imports. Note that the three solid arrows starting at $E_t$ do not represent independent thinnings as they are linked by the multinomial distribution and necessarily $L_{t, 1} + L_{t, 2} + A_t = E_t$.}
\label{fig:ingarch_flowchart_poisson_pq}

\end{figure}


\subsection{The compound Poisson INGARCH(1, 1) model}
\label{subsec:compound}

A compound Poisson INGARCH(1, 1) process $\{X_t, t \in \mathbb{Z}\}$ equivalent to \eqref{eq:N_CP_original}--\eqref{eq:lambda_CP_original} is obtained by extending \eqref{eq:L_t}--\eqref{eq:juv_t_v2} to
\begin{align}
L_t & = \beta \circ E_t \label{eq:L_t_CP}\\
A_t & = E_t - L_t \label{eq:A_t_CP}\\
C_t & = \kappa \star X_t \label{eq:C_t_CP}\\
X_t & = \psi *_G (I_t + A_t)\label{eq:X_t_CP} \\
E_t & = L_{t - 1} + C_{t - 1},\label{eq:juv_t_CP}
\end{align}
with $E_0 \sim \text{Pois}(\eta)$. The employed operator $*_G$ is defined as
$$
\psi *_G N = \sum_{i = 1}^N Z_i \ \ \ \text{ with } \ \ \ Z_i \stackrel{\text{ind}}{\sim} G(\psi)
$$
and thus denotes summing over independent samples from a secondary distribution $G(\psi)$. The parameters $\beta$ and $\psi$ as well as the type of the secondary distribution $G$ are shared across the two formulations. The remaining parameters of the original formulation can be recovered as
$
\nu = \mu_\psi(1 - \beta)\tau$ and $ \ \
\alpha = \mu_\psi(1 - \beta)\kappa.
%; \ \
% \lambda_0 = \mu_\psi \left(\tau + \frac{1 - \beta}{\beta} \times \eta\right)
$
Concerning the initialization, equivalence is achieved by setting $\lambda_0 = \mu_\psi\times \left\{\tau + (1 - \beta) \times \eta\right\}$.

The interpretation provided in Section \ref{subsec:poisson11} can be adapted as follows, see also Figure \ref{fig:ingarch_flowchart}:
\begin{itemize}
\item In the extended model $E_t$ does not represent exposed individuals, but clusters of exposed individuals. The number of individuals per cluster follows a $G(\psi)$ distribution, and all members of a cluster become infectious simultaneously.
\item Imports consist of a $\text{Pois}(\psi)$-distributed number $I_t$ of clusters, each of which releases a $G(\psi)$-distributed number of infectives into $X_t$.
\end{itemize}
The biological analogy is somewhat stretched here as in real life one would not expect all members of a cluster to become infectious at the same time. Note that the extension to the compound Poisson setting translates directly to higher-order models as discussed in the previous section, but we omit details.

\begin{figure}[h!]
\center
\includegraphics[scale = 0.8]{figure/flowchart_ingarch_cp_SEIR.pdf}
\caption{Interpretation of the CP-INGARCH(1, 1) formulation \eqref{eq:L_t}--\eqref{eq:juv_t_v2} as a stochastic epidemic process. Solid lines represent results of binomial thinning, dashed lines Poisson thinning, double lines thinning with $*_G$. Note that the two solid arrows starting at $E_t$ do not represent independent binomial thinnings as by construction  $L_t + A_t = E_t$.}
\label{fig:ingarch_flowchart}
\end{figure}

% Thinning-based formulations of INGARCH(1, 1) models also appear in \cite{Ferland2006} and \cite{Goncalves2015}. However, these take the form of successive approximations approaching the INGARCH(1, 1) in the limit and thus provide a less direct handle on stochastic properties of the process. 


\section{Derivation of some stochastic properties}

We now discuss the derivation of some stochastic properties of the considered models via their reformulation. We focus on the CP-INGARCH(1, 1) model and exploit a link to branching process theory.

\subsection{Geometric ergodicity}

Fokianos et al \cite{Fokianos2009} showed that a perturbed version of the Poisson INGARCH(1, 1) model is geometrically ergodic and Neumann \cite[Theorem 3.1]{Neumann2011} proved geometric ergodicity of a more general class of Poisson autoregressive models. Ergodicity of various other generalizations has been adressed by several authors \citep{Davis2016, Douc2013, Neumann2011}. Goncalves et al \citep{Goncalves2015} cover ergodicity (but not geometric ergodicity) of CP-INGARCH models. A proof of geometric ergodicity of the GP-INGARCH(1, 1) model along the lines of \cite{Neumann2011} has been sketched by Zhu \citep{Zhu2012}. Higher-order Poisson INGARCH models have been addressed more recently by Neumann \citep{Neumann2021}, while Doukhan et al \citep{Doukhan2021} treat a more general class not limited to conditional Poisson distributions. Mixing properties of non-stationary versions have been considered, too \cite{Doukhan2021a}. %  Other proofs of (geometric) ergodicity of INGARCH models have been given in the following works. Neumann \citep{Neumann2011} provides a contractive condition for ergodicity of a more general model class which includes non-linear autoregressive models, but also the Poisson INGARCH(1, 1). Zhu \citep{Zhu2012} states that the same technique can also be used to establish geomtric ergodicity of the GP-INGARCH(1, 1) model. Ergodicity of broader classes which are not limited to a conditional Poisson or generalized Poisson distribution have been considered by Douc et al \citep{Douc2013} and Davis and Liu \cite{Davis2016}. Goncalves et al \cite{Goncalves2015} established ergodicity (but not geometric ergodicity) of CP-INGARCH models. %, including higher-order models. % Multivariate Poisson INGARCH models have been analyzed by Fokianos et al \cite{Fokianos2020}.% However, to our best knowledge, geometric ergodicity has so far only been established for the Poisson INGARCH(1,1) model.\todo{Add Fokianos multivariate}

All of the mentioned contributions can be described as technically involved. As noted by Neumann \cite{Neumann2011}, the difficulty lies in the fact that the process $\{X_t\}$ is discrete-valued, while the conditional mean process $\{\lambda_t\}$ is real-valued. The re-formulation \eqref{eq:L_t_CP}--\eqref{eq:juv_t_CP} of the CP-INGARCH(1, 1) process involves exclusively integer-valued processes, and allows us to exploit results from branching process theory. Substituting $X_{t - 1} = \psi *_G I_t + \psi *_G A_t$ from equation \eqref{eq:X_t_CP} in \eqref{eq:juv_t_CP}, $\{\juv_t\}$ can be written as
\begin{align}
% \juv_t = \underbrace{L_{t - 1}}_{= \beta \circ E_{t - 1}} + \kappa \star \{\psi *_G (E_{t - 1} - L_{t - 1})\} + \kappa \star (\psi *_G I_{t - 1}). \label{eq:recursion_S}
\juv_t = \underbrace{L_{t - 1}}_{= \beta \circ E_{t - 1}} \ + \ \kappa \star (\psi *_G \underbrace{A_{t - 1}}_{ = E_{t - 1} - L_{t - 1}}) \ + \ \kappa \star (\psi *_G I_{t - 1}). \label{eq:recursion_S}
\end{align}
Here, $\kappa \star (\psi *_G I_{t - 1})$ represents exposed clusters caused by imported infections from $t - 1$. The term  $L_{t - 1}$ contains exposed clusters remaining from $ \juv_{t - 1}$, while $\kappa \star (\psi *_G A_{t - 1})$ are new exposed clusters caused by within-community transmission, i.e. by individuals who had been released from the exposed to the infectious pool in $t - 1$. Each exposed cluster from $E_{t - 1}$ thus contributes to $E_t$ in exactly one of two ways. Either it remains in the exposed pool itself, entering $E_t$ via $L_{t - 1}$, which happens with probability $\beta$. Or, with probability $1 - \beta$, it becomes infectious (enters into $A_{t - 1}$) and releases a $G(\psi)$-distributed number of newly infectious individuals into $X_{t - 1}$. These in turn each contribute new exposed clusters to $E_t$ according to a $\text{Pois}(\kappa)$ distribution. Denoting by $B_{k, t - 1}$ the number of clusters contributed to $E_t$ by the $k$-th of the $E_{t - 1}$ clusters from $t - 1$, we thus have
\begin{align}
B_{k, t - 1} & = \begin{cases}
1 & \text{with probability } \beta\\ % \ \ \ \ \ \ \  (\text{contribution via } L_{t - 1}) \\
\kappa \star (\psi *_G 1) & \text{with probability } 1 - \beta % \ \ (\text{ contribution via } \kappa \star (\psi *_G A_{t - 1})),
\label{eq:Z_t_i}
\end{cases}
\end{align}
independently for each $k = 1, \dots, E_{t - 1}$. Setting
\begin{align}
I^*_t & = \kappa\star (\psi *_G I_{t - 1}) \label{eq:I_star}
\end{align}
we can then re-write recursion \eqref{eq:recursion_S} as
$$
\juv_t = \sum_{k = 1}^{\juv_{t - 1}} B_{k, t - 1} \ \ + \ \ I^*_t.
$$
Thus, $\{\juv_t\}$ is a Galton-Watson branching process where the offspring distribution \eqref{eq:Z_t_i} is a specific one-inflated compound distribution. %, specifically, $A_{t, i}$ is a $G(\psi)$-stopped sum of Poisson random variables. 
The immigration distribution \eqref{eq:I_star} is defined by two compounding steps. % This follows from the fact that given $I_t$,  %are generated by two compounding steps. % and thus follow a Poisson Poisson compound or Neyman type A distribution with parameters $\kappa$ and $\tau$ \cite{Masse2005}.

Theory on branching processes with immigration, specifically Theorem 1 from Pakes \cite{Pakes1971} tells us that $\{\juv_t\}$ is geometrically ergodic if (a) $\mathbb{E}(B_{k, t}) < 1$, (b) $\mathbb{E}[(B_{k, t})\log(B_{k, t}) \ \mid \ B_{k, t} \geq 1] < \infty$, (c) $\mathbb{E}(I^*_t) < \infty$. These conditions are easily verified for $\{E_t\}$ provided that $\kappa\mu_\psi < 1$ and, as previously assumed, $\sigma^2_\psi < \infty$.
%\begin{itemize}
%\item[(a)] $\mathbb{E}(A_{i, t}) < 1$,
%\item[(b)] $\mathbb{E}[(A_{i, t} + 1)\log(A_{i, t} + 1)] < \infty$,
%\item[(c)] $\mathbb{E}(I^*_t) < \infty$.
%\end{itemize}
% Condition (a) holds if $\kappa\mu_\psi < 1$. For condition (b) note that $\mathbb{E}[(A_{i, t} + 1)\log(A_{i, t} + 1)] < \mathbb{E}(A_{i, t}^2)$. As $A_{i, t}$ comes from a mixture of two distributions which both have finite second moments (noting that $\text{Var}[\kappa \star (\psi *_G 1)] = \kappa(\mu_\psi + \sigma^2_\psi) < \infty$ \todo{re-check!}) we have $\mathbb{E}(A_{i, t}^2) < \infty$. Condition (c) holds as $\mathbb{E}(I^*_t) = \kappa\mu_\psi\tau < \infty$.

% (For completeness we note that $\text{Var}[\kappa \star (\psi *_G 1)] = \mathbb{E}[\text{Var}\{\kappa \star (\psi *_G 1)\} \ \mid \ \psi *_G 1] + \text{Var}[\mathbb{E}\{\kappa \star (\psi *_G 1)\}  \ \mid \ \psi *_G 1] = \kappa\mu_\psi $)

As in Fokianos et al \cite{Fokianos2009}, Proposition 1 from Meitz and Saikkonen \cite{Meitz2008} can then be used to show that geometric ergodicity of $\{E_t\}$ is inherited by the joint process $\{(\juv_t, L_t, A_t, X_t, C_t)\}$. %, see Appendix \ref{appendix:proof_meitz}.
Even though it is in principle sufficient to initialize the process with $E_0$ and $X_0$ as in Section \ref{subsec:compound}, we now assume that $\{(\juv_t, L_t, A_t, X_t, C_t)\}$ is initialized by a vector $(e_0, l_0, a_0, x_0, c_0)$ with all elements from $\mathbb{N}_0$ and $l_0 + a_0 = e_0, x_0 \geq a_0$. Geometric ergodicity of the joint process is then established by verifying two conditions (Assumption 1 in \cite{Meitz2008}):

\begin{enumerate}
% \item Given $(\juv_u, L_u, A_u, X_u, C_u), u < t$ and $\juv_t$, $(L_t, A_t, X_t, C_t)$ depends only on $\juv_t$. This is straightforward to see from equations \eqref{eq:juv_t}--\eqref{eq:C_t} or Figure \ref{fig:ingarch_flowchart}.
\item Given $E_t$, $(L_t, A_t, X_t, C_t)$ is independent of all $\juv_u, L_u, A_u, X_u, C_u, u < t$. It is straightforward to see from equations \eqref{eq:L_t_CP}--\eqref{eq:juv_t_CP} or the graphical representation in Figure \ref{fig:ingarch_flowchart} that this is the case.
\item There is an $n \geq 1$ such that for all $t > n$, the generation mechanism of $\juv_t \ \mid \ \juv_0 = e_0, L_0 = l_0, A_0 = a_0, X_0 = x_0, C_0 = c_0$ has the same structure as that of $\juv_t \ \mid \ \juv_n = \tilde{e}_n$, where $\tilde{e}_n$ is some function of $(e_0, l_0, a_0, x_0, c_0)$. As $(e_0, l_0, a_0, x_0, c_0)$ only impacts the further course of the process $\{\juv_t\}$ through $\juv_1 = c_0 + l_0$, this is the case for $n = 1$. % and $\tilde{s}_1 = c_0 + l_0$.
\end{enumerate}
% This proves geometric ergodicity of the joint process $\{(\juv_t, L_t, X_t, C_t\}$ if $\kappa\mu_\psi < 1$ (and $\sigma_\psi^2 < \infty$, as assumed since Section \ref{sec:original_formulation}).

\subsection{Existence of higher moments}

Existence of higher-order moments of CP-INGARCH(1, 1) models with time-constant $\psi$ has been proven in \cite{Silva2016}, but the proof is quite involved. Representation \eqref{eq:L_t_CP}--\eqref{eq:juv_t_CP} allows again for a more condensed argument. It is known that if the offspring and immigration distributions of a subcritical Galton-Watson branching process have finite $r$-th moments, this is also the case for the limiting-stationary distribution \cite[Sec. 4]{Lange1981}. Both conditions are fulfilled for $\{E_t\}$ if the $r$-th moment of $G(\psi)$ is finite and $\kappa\mu_\psi < 1$:

\begin{itemize}
\item A randomly stopped sum $Y = \sum_{i = 1}^N Z_i$ of i.i.d. random variables $Z_i$ has finite $r$-th moments if $N$ and the $Z_i$ have finite $r$-th moments \cite[Theorem 5.2]{Gut2009}. The Poisson distribution has finite moments of any order. Thus, if the $r$-th moment of $G(\psi)$ is finite, this also holds for $\kappa \star (\psi *_G 1)$ and thus the offspring distribution from \eqref{eq:Z_t_i}.
\item Similar arguments imply that if $G(\psi)$ has a finite $r$-th moment, this is also the case for $\psi *_G I_{t - 1}$ and in a second step the immigration process $I^*_t = \kappa \star(\psi *_G I_{t - 1})$ from equation \eqref{eq:I_star}.
\end{itemize}
Consequently, the limiting-stationary distribution of $\{E_t\}$ has finite moments up to order $r$ if this is the case for the secondary distribution $G(\psi)$ (while $\kappa\mu_\psi < 1$). It is then straightforward to show that finiteness of moments translates to $A_t = (1 - \beta) \circ E_t$, $\psi *_G A_t$ and ultimately $X_t = \psi *_G I_t + \psi *_G A_t$.
%
%\subsection{Consistency and asymptotic normality of moment estimators}
%
%Use same argument as in Weiss and Schweer 2016, exploiting Ibragimov 1962. Derive stationary mean, variance, acf for (1, 1) case, should be straightforward. Maybe even try getting the elements of the covariance matrix.

\subsection{Approximating the limiting-stationary distribution}

As the CP-INGARCH(1, 1) process $\{X_t\}$ is ergodic, the limits % \todo{Add example with computation times? Distinction of h-step ahead distributions and using matrix inverse; possibly check setting like $\alpha = 0.2, \beta = 0.75$.}
$$
p_i = \lim_{t \rightarrow \infty} \text{Pr}(X_t = i \ \mid (e_0, l_0, a_0, x_0, c_0))
$$
exist and are independent of the initialization of the process (in the following we therefore suppress the condition in the notation). However, even in the Poisson case no closed form for the $p_i$ is known. For the INARCH(1) model, a Markov chain approach can serve to approximate the $p_i$ with arbitrary precision \cite{Weiss2010}. This is based on the fact that the INARCH(1) is a discrete first-order Markov chain so that
$$
\text{Pr}(X_t = i) = \sum_{j = 0}^\infty p_{j|i} \times \text{Pr}(X_{t - 1} = j).
$$
Here, $p_{j|i} = \text{Pr}(X_t = i \ | \ X_{t - 1} = j) $ is the transition probability from $j$ to $i$. Choosing $T$ sufficiently large, the approximation is given by
$$
p_i \approx \text{Pr}(X_T = i),
$$
where $\text{Pr}(X_T = i)$ can in turn be approximated by recursively computing
$$
\text{Pr}(X_t = i) \approx \sum_{j = 0}^M p_{j|i} \times \text{Pr}(X_{t - 1} = j),
$$
with sufficiently large support $M$ and some suitable initialization of $X_0$ (e.g. starting close to the stationary mean).

This method, however, is not directly applicable to the INGARCH(1, 1) model \eqref{eq:N_CP_original}--\eqref{eq:lambda_CP_original} as $\{X_t\}$ is not a first-order Markov chain. Application to the joint process $\{(X_T, \lambda_t)\}$, which is a first-order Markov chain, is not feasible as $\lambda_t$ has a continuous support. Formulation \eqref{eq:L_t}--\eqref{eq:juv_t_v2}, however, enables us to apply the approximation
$$
p^{(E)}_i \approx \text{Pr}(E_T = i) \ \ \text{and} \ \ \text{Pr}(E_t = i) \approx \sum_{j = 0}^M p^{(E)}_{j|i} \times \text{Pr}(E_{t - 1} = j)
$$
with large $T$ and $M$ to the discrete-valued Markov chain $\{E_t\}$. The computation of the transition probabilities $p_{j|i}$ based on recursion \eqref{eq:recursion_S} is slightly tedious, but without conceptual difficulty. Subsequently one can compute an approximation of the limiting stationary distribution of $X_t$ as
$$
p_i = \sum_{k = 0}^M p_k^{(E)} \times \text{Pr}(X_t = i \ \mid \ E_t = k).
$$
% While it is somewhat tedious to compute the transition probabilities $p^{(E)}_{j|i}$ of $\{E_t\}$ as well as the probabilities $\text{Pr}(X_t = i \ \mid \ E_t = k)$ linking $\{E_t\}$ and $\{X_t\}$, this approach allows to evaluate the $p_t$ with arbitrary precision.


%% Existence of three derivatives of the log likelihood function is shown by Fokianos et al. Then Jensen and Rahbek's law of large numbers for geometrically ergodic processes implies normality of the maximum likelihood estimators.

% \section{Discussion}

% We have provided an alternative representation of the compound \todo{to be extended} Poisson INGARCH(1, 1) model and used it to obtain several properties of the process. Notably, the fact that the process is geometrically ergodic could be useful for studies of the behaviour of conditional maximum likelihood estimators along the lines of \cite{Fokianos2009} and \cite{Zhu2012}. % This question has not yet been addressed as existing studies are limited to conditional least squares (CLS) estimators \cite{Silva2016} and Poisson quasi-maximum likelihood estimators \cite{AhmadXXX}.

% Mention that CLS estimator is normal according to Cardoso da Silva. PQML is normal according to Ahmad and Francq.

%\section{Moment-based estimation}
%
%Limiting stationary properties: Set $\xi = \alpha + \beta$ and denoting the conditional dispersion index of $X_t \ \mid \ \lambda_t$ by
%$$
%\delta = \frac{\text{Var}(X_t \ \mid \ \lambda_t)}{\lambda_t} = \frac{\sigma^2_\psi + \mu_\psi^2}{\mu_\psi}.
%$$
%Then:
%$$
%\mu = \frac{\nu}{1 - \xi}, \ \ \sigma^2 = \frac{1 - \xi^2 + \alpha^2}{1 - \xi^2} \cdot \delta\mu, \ \ \rho(d) = \alpha\cdot \frac{1 - \beta\xi}{1 - \xi^2 + \alpha^2}\cdot \xi^{d - 1}
%$$
%
%Moment estimators:
%\begin{align*}
%\hat{\xi} & = \frac{\hat{\rho}(2)}{\hat{\rho}(1)}\\
%\hat{\nu} & = \hat{\mu}\cdot(1 - \hat{\xi})\\
%\hat{\alpha} & = \frac{(1 - \hat{\xi}^2) - \sqrt{(1 - \hat{\xi}^2) \cdot [1 - \hat{\xi}^2 - 4\hat\rho(1)\cdot\{\hat\rho(1) - \hat{\xi})\}]}}{2\{\hat\rho(1) - \hat{\xi}\}}\\
%\hat{\beta} & = \hat\xi - \hat\alpha\\
%\hat{\delta} & = \frac{\hat\sigma^2}{\hat\mu} \cdot \frac{1 - \hat\xi^2}{1 - \hat\xi^2 + \hat\alpha^2}
%\end{align*}
%
%
%For Poisson case one might derive the precise expressions.

% \newpage

\appendix
\section{Equivalence of the classical and thinning-based INGARCH formulations}
\label{appendix:proof}

\subsection{Poisson INGARCH(1, 1)}
\label{subsec:derivation_poisson11}

We demonstrate that the process $\{X_t, t \in \mathbb{N}_0\}$ from \eqref{eq:L_t}--\eqref{eq:juv_t_v2} is equivalent to the Poisson INGARCH(1, 1) process \eqref{eq:X_t_original}--\eqref{eq:lambda_t}. We start by decomposing $C_t, t \in \mathbb{N}_0$ and $E_0$ by when these exposed individuals will become infectious. We denote by $C_t^{(i)}$ the number of exposed persons caused by infectives from time $t$ and turning themselves infectious at $t + i$; and by $E^{(i)}_0$ the number of exposed individuals initially in the pool and turning infectious at time $i$ (note that $C_t^{(i)}$ is not to be confused with $C_{t, i}$ from Section \ref{subsec:poissonpq}). This implies
\begin{equation}
% C_t = \sum_{i = 1}^\infty C_t^{(i)}, \ \ \ 
% L_0 = \sum_{i = 1}^\infty L_0^{(i)}, \ \ \ 
A_t = \sum_{i = 1}^{t} C_{t - i}^{(i)} \ \ + \ \ E_0^{(t)}.
% L_t & = \sum_{i = 1}^t \sum_{j > i} C_{t - i}^{(j)} \ \ + \ \ \sum_{j > t} L_0^{(j)}
\label{eq:sums}
\end{equation}
for $ t \geq 1$. A person contaminated by an infective from time $t$ (entering the exposed pool at time $t + 1$) has a probability of 
\begin{equation}
\beta^{i - 1}(1 - \beta)\label{eq:geom_distr}
\end{equation}
to become infectious at time $t + i, i = 1, 2, \dots$, and thus be part of $C_t^{(i)}$ (it has to remain in the exposed pool $i - 1$ times and then turn infectious). The Poisson splitting property \cite{Kingman1993} then implies that given $X_t$, the $C_t^{(i)}, i =1, 2, \dots$ are independently Poisson distributed,
$$
C_t^{(i)} \mid X_t \stackrel{\text{ind}}{\sim} \text{Pois}(\beta^{i - 1}[1 - \beta]\kappa X_t), i =1, 2, \dots % ; \ \ C_t^{(i)} \perp C_t^{(j)} \mid X_t, i \neq j.
$$
We note that given $X_t$, $C_t^{(i)}$ does not have any impact on the further course of the process $\{X_t\}$ until time $t + i$. Also, given $X_t$, $C_t^{(i)}$ is independent of all preceding values $X_{t - 1}, X_{t - 2}, X_0$. We can thus extend the condition in the above and write
\begin{equation}
C_t^{(i)} \mid X_{t + i - 1}, \dots, X_0 \sim \text{Pois}(\beta^{i - 1}[1 - \beta]\kappa X_t). \label{eq:conditional_Cti} % ; \ \ C_t^{(i)} \perp C_t^{(j)} \mid X_t, i \neq j
\end{equation}
Moreover, again because, given $X_t$, $C_t^{(i)}$ only impacts the further process from $t + i$ onwards, it is clear that $C_t^{(i)} \perp C_u^{(j)} \mid X_{t + i - 1}, X_{t + i - 2}, \dots, X_0$ if $u < t + i$.

Now consider %\todo{there seems to be a slight switch in what index $t$ represents}
\begin{align}
X_t & = I_t \ \ + \ \ \underbrace{\sum_{i = 1}^{t} C_{t - i}^{(i)} \ \ + \ \ E_0^{(t)}}_{= A_t}, \label{eq:decomposition_Xt}
\end{align}
where we substituted $A_t$ in equation \eqref{eq:X_t_v2} using equation \eqref{eq:sums}. In analogy to the above argument, $E_0^{(t)}$ is Poisson distributed with rate $\beta^{t}(1 - \beta)\eta$ and independent of all $X_u$ and $C_u^{(i)}, u \leq t - 1$. Conditioned on $X_{t - 1}, \dots, X_0$, we thus have that $X_t$ is a sum of independent Poisson random variables. This implies
$$
X_t \mid X_{t - 1}, \dots, X_0 \sim \text{Pois}(\lambda_t)
$$
where the conditional expectation is given by
\begin{align*}
& \lambda_t = \mathbb{E}(I_t) \ \ + \ \ \sum_{i = 1}^t \mathbb{E}(C_{t - i}^{(i)} \ \mid \ X_{t- 1}, \dots, X_0) \ \ + \ \ \mathbb{E}(E_0^{(t)})\\
& \ \ \ = \ \ \tau \ \ \ \ \ + \ \ \sum_{i = 1}^t \beta^{i - 1}(1 - \beta)\kappa X_{t - i} \ \ \ \ \ \ \ \ \ \ + \ \ \beta^{t}(1 - \beta)\eta.\\
\end{align*}
We can then re-write $\lambda_t$ as
\begin{align*}
\lambda_t & = (1 - \beta)\tau + (1 - \beta)\kappa X_{t - 1} + \beta \left\{\tau +    \sum_{i = 2}^t \beta^{i - 1}(1 - \beta)\kappa X_{t - i}  \ \ + \ \ \beta^{t - 1}(1 - \beta)\eta\right\}\\
& = \underbrace{(1 - \beta)\tau}_{\nu} \ + \ \underbrace{(1 - \beta)\kappa}_{\alpha} X_{t - 1} \ + \ \beta \lambda_{t - 1}
\end{align*}
for $t \geq 2$. This is the form a Poisson INGARCH(1, 1) model. We conclude by considering the initialization of the process, where we have
\begin{align*}
\lambda_1 = \mathbb{E}(X_1 \ \mid \ X_0) & = \tau + (1 - \beta)\kappa X_0 + \beta(1 - \beta)\eta\\
& =  (1 - \beta)\tau + (1 - \beta)\kappa X_0 + \beta  \left\{\tau + (1 - \beta) \times \eta \right\},
\end{align*}
meaning that we have to set $\lambda_0 =\tau + (1 - \beta) \times \eta$ for initialization. The proofs for the INGARCH($p, q$) and CP-INGARCH(1, 1) models follow the same structure and have been moved to the Supplementary Material for space reasons. 

%\subsection{V2}
%
%We demonstrate that the process $\{X_t, t \in \mathbb{N}_0\}$ from \eqref{eq:L_t}--\eqref{eq:juv_t_v2} is equivalent to the Poisson INGARCH(1, 1) process \eqref{eq:X_t_original}--\eqref{eq:lambda_t}. We start by decomposing $C_t, t \in \mathbb{N}_0$ and $L_0$ by when the respective eggs will hatch. We denote by $C_t^{(i)}$ the number of eggs laid at time $t$ and hatching at $t + i$ and by $R^{(i)}_0$ the number of eggs initially in the stock and hatching at time $i$ (note that these are not to be confused with $C_{t, i}$ and $L_{o, i}$ from Section \ref{subsec:poissonpq}). This implies
%\begin{equation}
%% C_t = \sum_{i = 1}^\infty C_t^{(i)}, \ \ \ 
%% L_0 = \sum_{i = 1}^\infty L_0^{(i)}, \ \ \ 
%A_t = \sum_{i = 1}^{t} C_{t - i}^{(i)} \ \ + \ \ L_0^{(t)}.
%% L_t & = \sum_{i = 1}^t \sum_{j > i} C_{t - i}^{(j)} \ \ + \ \ \sum_{j > t} L_0^{(j)}.
%\label{eq:sums}
%\end{equation}
%Denote the probability that an egg laid at time $t$ hatches at time $t + i, i = 1, 2, \dots$, thus becoming part of $C_t^{(i)}$, by $p_i$; it is easy to show that $p_i = \beta^i(1 - \beta)$. The Poisson splitting property \cite{Kingman1993} then implies that given $X_t$, the $C_t^{(i)}$ are independently Poisson distributed,
%$$
%C_t^{(i)} \mid X_t \stackrel{\text{ind}}{\sim} \text{Pois}(p_i\kappa X_t). % ; \ \ C_t^{(i)} \perp C_t^{(j)} \mid X_t, i \neq j.
%$$
%We note that given $X_t$, $C_t^{(i)}$ does not have any impact on the further course of the process $\{X_t\}$ until time $t + i$. Also, given $X_t$, $C_t^{(i)}$ is independent of the past of $\{X_t\}$. We can thus extend the condition in the above and write
%$$
%C_t^{(i)} \mid X_{t + i - 1}, \dots, X_0 \stackrel{\text{ind}}{\sim} \text{Pois}(p_i\kappa X_t). % ; \ \ C_t^{(i)} \perp C_t^{(j)} \mid X_t, i \neq j
%$$
%Moreover, again because, given $X_t$, $C_t^{(i)}$ only impacts the further process from $t + i$ onwards, it is clear that $C_t^{(i)} \perp C_u^{(j)} \mid X_t, X_u$ for $t < u < t + i$.
%
%Now consider %\todo{there seems to be a slight switch in what index $t$ represents}
%\begin{align}
%X_t & = I_t \ \ + \ \ \underbrace{\sum_{i = 1}^{t} C_{t - i}^{(i)} \ \ + \ \ L_0^{(t)}}_{= A_t}, \label{eq:introduce_N_T}
%\end{align}
%where we substituted $A_t$ in equation \eqref{eq:X_t_v2} using equation \eqref{eq:sums}. In analogy to the above argument, $L_0^{(t)}$ is Poisson distributed with rate $p_t\eta$ and independent of all $X_u$ and $C_u^{(i)}, u \leq t - 1$. Conditioned on $X_{t - 1}, \dots, X_0, \eta$, we thus have that the term $X_t$ is a sum of independent Poisson random variables (as $I_t \sim \text{Pois}(\tau)$). This implies
%$$
%X_t \mid X_{t - 1}, \dots, X_0, \eta \sim \text{Pois}(\lambda_t)
%$$
%with some $\lambda_t$, which we will determine in the next step. To this end, denote by $\xi_{t} = \mathbb{E}(E_t  \ \mid \ X_{t - 1}, \dots, X_0)$. We note that for $t = 2, 3, \dots$
%\begin{align}
%\lambda_t & = (1 - \beta)\xi_t + \tau
%\label{eq:EXt}\\
%\xi_t & = \beta \times \xi_{t - 1} + \kappa X_t.
%\label{eq:ESt}
%\end{align}
%The latter recursion is a direct consequence of the fact that $p_{i + 1} = \beta p_i$ and thus $\mathbb{E}(L^{(i + 1)}_t \ \mid \ X_t) = \beta\mathbb{E}(L^{(i)}_t \ \mid \ X_t)$ and $\mathbb{E}(R^{(t + 1)}_0) = \beta\mathbb{E}(R^{(t)}_0)$. Plugging \eqref{eq:EXt} into \eqref{eq:ESt} we then obtain
%$$
%\frac{\lambda_t - \tau}{1 - \beta} = \beta \times \frac{\lambda_{t - 1} - \tau}{1 - \beta} + \kappa X_{t - 1}
%$$
%which simplifies to
%$$
%\lambda_t = \underbrace{(1 - \beta)\tau}_{\nu} \ + \ \underbrace{(1 - \beta)\kappa}_{\alpha} X_{t - 1} \ + \ \beta \lambda_{t - 1}.
%$$
%This is the form a Poisson INGARCH(1, 1) model. We conclude by considering the initialization of the process, where we have
%\begin{align*}
%\lambda_1 = \mathbb{E}(X_1 \ \mid \ X_0) & = \tau + (1 - \beta)\kappa X_0 + (1 - \beta)\eta\\ % \lambda_1 = \tau(1 - \beta) + (1 - \beta)\kappa X_0 + \beta\left(\tau + \frac{1 - \beta}{\beta} \times \eta \right),\\
%& =  \tau (1 - \beta) + (1 - \beta)\kappa X_0 + \beta  \left(\tau + \frac{1 - \beta}{\beta} \times \eta \right),
%\end{align*}
%meaning that we have to set $\lambda_0 =\tau + (1 - \beta)/\beta \times \eta$.
%
%\subsection{Poisson INGARCH($p, q$)}
%
%We now denote by
%$$
%C_t = \sum_{i = 1}^p C_{t, i}
%$$
%the total number of eggs laid at time $t$ (summed over the different entry times into $\{E_t\}$). This quantity can also be decomposed by when the eggs hatch. Denote as in the prevous section by $C_t^{(i)}$ the number of eggs laid at $t$ and hatching at $t + i$. We now decompose these even further and denote by $C_t^{(i, j)}, i \geq 1, j = 1, \dots, q$ the number of eggs laid at $t$, hatching at $t + i$ and having moved into $E_{t + i}$ directly from $E_{t + i - j}$ (i.e., having been part of $L_{t + i - j}^{(j)}$ in equation \eqref{eq:L_t_mult}; see interpretation in Section \ref{subsec:poissonpq}). We thus have the rather fine decomposition
%$$
%C_t = \sum_{i = 1}^\infty C_t^{(i)} = \sum_{i = 1}^\infty \sum_{j = 1}^q C_t^{(i, j)}
%$$
%
%The same decomposition is defined for the eggs entering the stock at initialization. We denote by $E_u^{(i)}, u = -q + 1, \dots, 0$ the number of eggs entering the process via $E_u$, hatching at $u + i$; and by $E_u^{(i, j)}, u = -q + 1, \dots, 0$ the number of eggs entering the process via $E_u$, hatching at $u + i$ and having been passed to $E_{u + i}$ from $E_{u + i - j}$ via $L_{u + i - j, j}$.
%
%This allows us to write
%\begin{align}
%A_t & = \sum_{i = 1}^{t + q - 1} L^{(i)}_{t - i} \ \ + \ \ \sum_{j = -q + 1}^{0} E_{j}^{(t - j)} \nonumber\\
%& = \sum_{i = 1}^{t + q - 1} \sum_{k = 1}^q L^{(i, k)}_{t - i} \ \ + \ \ \sum_{j = -q + 1}^{0} \sum_{k = 1}^q E_{j}^{(t - j, k)}.\label{eq:Htpq}
%\end{align}
%
%Paralleling the arguments form the previous section it can be shown that given $X_{t - 1}, \dots, X_{-q + 1}$ all summands in \eqref{eq:Htpq} are independent Poisson random varianbles. This implies that $A_t$ and thus also $X_t = I_t + A_t$ are conditionally Poisson with a rate $\lambda_t$, which we will address in the next step.
%
%As in the previous section we consider the conditional expectation of $E_t$,
%$$
%\xi_t = \mathbb{E}(E_t \ \mid \ X_{t - 1}, \dots, X_{1 - p}),
%$$
%and note that
%$$
%\lambda_t = \left(1 - \sum_{j = 1}^q \beta_j \right) \times \xi_t.
%$$
%It is straightforward to see that
%$$
%\mathbb{E}(C_t^{i, j})
%$$
%
%  
%\begin{equation}
%\lambda_t = (1 - \beta)\xi_t + \tau
%\end{equation}
%and
%\begin{equation}
%\xi_t = \sum_{i = 1}^p \kappa_i X_{t - i} + \sum_{j = 1}^p \beta_i\xi_{t - j}
%\end{equation},
%which after some algebra leads to

% \newpage

% \subsection{Poisson INGARCH($p, q$)}
% \label{subsec:derivation_poissonpq}

% The proof follows the same steps as in Section \ref{subsec:derivation_poisson11}, but is somewhat lengthy due to the more complex recursive relationships. It has therefore been moved to the Supplementary Material.


\section*{Acknowledgements}

I would like to thank Konstantinos Fokianos and Christian Wei{\ss} for helpful discussions.


% \section*{References}
{
\footnotesize
\bibliographystyle{plain}
\bibliography{bib_ingarch.bib}
}
% \newpage


\section{Supplementary material} % for \textit{A thinning-based representation of INGARCH models}}

\subsection{Demonstration of equivalence for the INGARCH($p, q$) model}

We use an argument similar to the one from Section \ref{subsec:derivation_poisson11} to demonstrate that the classical formulation \eqref{eq:X_t_original}, \eqref{eq:lambda_t_pq} of the INGARCH($p, q$) model and the thinning-based version \eqref{eq:Xt_thinning_pq}--\eqref{eq:L_t_mult} are equivalent. Again we denote by $C_t^{(i)}, i = 1, 2, \dots$ the number of persons contaminated by infectives from time $t$ and becoming themselves infectious at $t + i$. Extending on the notation from the Poisson INGARCH(1, 1) case, we denote by $E^{(i)}_m, m = 1 - q, \dots, 0, i = 1, 2, \dots$ the number of individuals entering the exposed pool via the initialization at time $m$ and turning infectious at time $m + i$. Generalizing equation \eqref{eq:decomposition_Xt} we then have
$$
X_t = I_t \ \ + \ \ \underbrace{\sum_{i = 1}^{t + p - 1} C_{t - i}^{(i)} \ \ + \ \ \sum_{m = 1 - q}^0 E_m^{(t - m)}}_{= A_t},
$$
for $t = 1, 2, \dots$. Arguments identical to those from the previous section imply that given $X_{t - 1}, \dots, X_{1 - p}$ all summands in the above equation are independently Poisson distributed, so that $X_t$, too, is conditionally Poisson with a rate $\lambda_t$.

Paralleling equation \eqref{eq:conditional_Cti}, the conditional expectation of $C_t^{(i)}$ is given by
\begin{equation}
\mathbb{E}(C_t^{(i)} \ \mid \ X_{t + i - 1}, \dots, X_{1 - p}) = \left(1 - \sum_{l = 1}^q \beta_l \right) \times \left(\sum_{k = 1}^p\ \kappa_k \pi_{i - k}\right) X_t,\label{eq:ELt}
\end{equation}
where we denote by $\pi_j$ the probability that an individual entering the exposed pool at time $t$ is also in the pool at time $t + j$. The reasoning behind this relationship is that the $X_t$ infectives from time $t$ generate exposures entering at times $t + 1, \dots, t + p$ with rates $\kappa_1, \dots, \kappa_p$, respectively. The exposed individuals then have to also be present in the exposed pool exactly $i - 1, \dots, i - p$ time points later, respectively (which happens with probabilities $\pi_{i - 1}, \dots, \pi_{i - p}$), and then leave it (which happens with probability $1 - \sum_{l = 1}^q \beta_l$).

For the $\pi_j$, the recursion
\begin{align}
\pi_j = \sum_{l = 1}^q \beta_l \pi_{j - l}, \label{eq:recursion_pi}
\end{align}
with $\pi_0 = 1$ and $\pi_k = 0$ for $k < 0$ holds. This is because an individual which entered the exposed pool at time $t$ can arrive in $E_{t + j}, j \geq 1$ by a move from any of $E_{t + j - 1}, \dots E_{t + j - q}$ (even though some of these moves may not be possible if $j < q$; this will be reflected in $\pi_{j - l} = 0$). To do so, the individual needs to have arrived at the respective $E_{t + j - l}$ (which it does with probability $\pi_{j - l}$) and then make an $l$-step jump into $E_{t + j}$ (this happens with probability $\beta_l$).

We can now consider
\begin{align}
\lambda_t = \mathbb{E}(X_t \ \mid \ X_{t - 1}, \dots, X_{1 - p}) = & \ \tau 
\ + \ \sum_{i = 1}^{t + p - 1}\mathbb{E}(L^{(i)}_{t - i} \ \mid \ X_{t - 1}, \dots, X_{1 - p})
\ + \sum_{m = 1 - q}^0 \mathbb{E}(E_{m}^{(t + m)}  \ \mid \ X_{t - 1}, \dots, X_{1 - p}).\label{eq:lambda_t_pq_recursion1}
\end{align}

Focusing on the second summand and plugging in equation \eqref{eq:ELt}, we obtain
\begin{align*}
\sum_{i = 1}^{t + p - 1}\mathbb{E}(L^{(i)}_t \ \mid \ X_{t - 1}, \dots, X_{1 - p}) = & \left(1 - \sum_{l = 1}^q \beta_l\right) \times \left( \sum_{i = 1}^{t + p - 1} \sum_{k = 1}^p \kappa_k \pi_{i - k} X_{t - i}\right)\\
= & \left(1 - \sum_{l = 1}^q \beta_l\right) \times \left(\sum_{k = 1}^p \sum_{i = k}^{t + p - 1} \kappa_k \pi_{i - k} X_{t - i}\right).
\end{align*}
Note that in the last step we can start the last sum from $i = k$ rather than $i = 1$ as $\pi_{i - k} = 0$ for $i < k$. We can then further decompose this sum into
\begin{align}
= & \left(1 - \sum_{l = 1}^q \beta_l\right) \times \left(\underbrace{\sum_{k = 1}^p \kappa_k \pi_0 X_{t - k}}_{\text{corresponds to } i = k; \text{ note: } \pi_0 = 1} \ + \ \sum_{k = 1}^p \sum_{i = k + 1}^{t + p - 1} \kappa_k \pi_{i - k} X_{t - i}\right)\nonumber\\
= & \left(1 - \sum_{l = 1}^q \beta_l\right) \times \left\{\sum_{k = 1}^p \kappa_k X_{t - k} \ + \ \sum_{k = 1}^p \sum_{i = k + 1}^{t + p - 1} \kappa_k \times \underbrace{\left(\sum_{j = 1}^q \beta_j \pi_{i - k - j}\right)}_{\text{using equation \eqref{eq:recursion_pi}}} \times X_{t - i}\right\}\nonumber\\
= & \left(1 - \sum_{l = 1}^q \beta_l\right) \times \left\{\sum_{k = 1}^p \kappa_k X_{t - k} \ + \ \sum_{j = 1}^q \beta_j \times \left(\sum_{k = 1}^p \sum_{i = k + 1}^{t + p - 1} \kappa_k \times \pi_{i - k - j} \times X_{t - i}\right)\right\}\nonumber\\
= & \left(1 - \sum_{l = 1}^q \beta_l\right) \times \left\{\sum_{k = 1}^p \kappa_k X_{t - k} \ + \ \sum_{j = 1}^q \beta_j \times \left(\sum_{k = 1}^p \sum_{i = k + 1 - j}^{(t - j) + p - 1} \kappa_k \times \pi_{i - k} \times X_{(t - j) - i}\right)\right\}\nonumber\\
= & \left(1 - \sum_{l = 1}^q \beta_l\right) \times \left\{\sum_{k = 1}^p \kappa_k X_{t - k} \ + \ \sum_{j = 1}^q \beta_j \times \left(\sum_{k = 1}^p \sum_{i = 1}^{(t - j) + p - 1} \kappa_k \times \pi_{i - k} \times X_{(t - j) - i}\right)\right\}\label{eq:substitution_ELt}
\end{align}
where in the last step we can let the last sum start at $i = 1$ rather than $i = k + 1 - j$ as $\pi_{i - k} = 0$ for $i = 1, \dots, k - j$.

For the third term from equation \eqref{eq:lambda_t_pq_recursion1} we pursue a similar recursive argument:
\begin{align}
\sum_{m = 1 - q}^0 \mathbb{E}(E_{m}^{(t + m)}  \ \mid \ X_{t - 1}, \dots, X_{1 - p}) & = \left(1 - \sum_{l = 1}^q \beta_l\right) \times \sum_{m = 1 - q}^0 \pi_{t - m}\rho_m\nonumber\\
& = \left(1 - \sum_{l = 1}^q \beta_l\right) \times \sum_{m = 1 - q}^0 \sum_{j = 1}^q \beta_j \pi_{t - j - m}\rho_m\nonumber\\
& =  \left(1 - \sum_{l = 1}^q \beta_l\right) \times\sum_{j = 1}^q \beta_j \times \left(\sum_{m = 1 - q}^0 \pi_{(t - j) - m}\rho_m\right).\label{eq:substitution_ESt}
\end{align}
Plugging the terms from \eqref{eq:substitution_ELt} and \eqref{eq:substitution_ESt} into \eqref{eq:lambda_t_pq_recursion1} we then get

\begin{align*}
\lambda_t = & \ \ \tau \ + \ \left(1 - \sum_{l = 1}^q \beta_l\right) \times \Bigg\{\sum_{k = 1}^p \kappa_k X_{t - k} \ + \ \sum_{j = 1}^q \beta_j \times \left(\sum_{k = 1}^p \sum_{i = 1}^{(t - j) + p - 1} \kappa_k \times \pi_{i - k} \times X_{(t - j) - i}\right) \\
& \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ + \ \sum_{j = 1}^q \beta_j \times \left(\sum_{m = 1 - q}^0 \pi_{(t - j) - m}\rho_m\right)\Bigg\}.
\end{align*}

This can be re-ordered to
\begin{align*}
\lambda_t & = \left(1 - \sum_{l = 1}^q \beta_l\right) \times \tau \ +  \ \left(1 - \sum_{l = 1}^q \beta_l\right) \times \left(\sum_{k = 1}^p \kappa_k X_{t - k}\right)\\
& \ \ \ + \sum_{j = 1}^q \beta_j \underbrace{\left\{\tau \ + \ \left(1 - \sum_{l = 1}^q \beta_l\right) \times \left(\sum_{k = 1}^p \sum_{i = 1}^{(t - j) + p - 1} \kappa_k \times \pi_{i - k} \times X_{(t - j) - i}\right) \ + \ \left(1 - \sum_{l = 1}^q \beta_l\right) \times \left(\sum_{m = 1 - q}^0 \pi_{(t - j) - m}\rho_m \right) \right\}}_{= \lambda_{t - j}}\\
& = \nu \ \ + \ \ \sum_{k = 1}^p \alpha_k X_{t - k} \ \ + \ \ \sum_{j = 1}^q \beta_j \lambda_{t - j},
\end{align*}
where
$$
\nu = \left(1 - \sum_{l = 1}^q \beta_l\right) \times \tau, \ \ \ \alpha_k = \left(1 - \sum_{l = 1}^q \beta_l\right) \times \kappa_k, k = 1, \dots, p.
$$
This is the form of a Poisson INGARCH($p, q$) model as defined in equation \eqref{eq:lambda_t_pq}. Concerning the initialization, it can be shown that one needs to set $\lambda_m = (1 - \sum_{j = 1}^q\beta_j) \times \eta_m, m = 1 - q, \dots, 0$. This can be done using essentially the same argument as in Section \ref{subsec:derivation_poisson11}, but we omit the somewhat lengthy details.

\subsection{Compound Poisson INGARCH(1, 1)}

Setting $N_t = I_T + A_t$, the same arguments as in \eqref{subsec:derivation_poisson11} can be used to show that
$$
N_t \mid X_{t - 1}, \dots, X_0 \sim \text{Pois}(\lambda_t/\mu_\psi)
$$
where the conditional expectation is given by
\begin{align*}
& \lambda_t/\mu_\psi = \mathbb{E}(I_t) \ \ + \ \ \sum_{i = 1}^t \mathbb{E}(C_{t - i}^{(i)}) \ \ + \ \ \mathbb{E}(E_0^{(t)})\\
& \ \ \ \ \ \ \ \ = \tau \ \ + \ \ \sum_{i = 1}^t \beta^{i - 1}(1 - \beta)\kappa X_{t - i} \ \ + \ \ \beta^{t}(1 - \beta)\eta\\
\Leftrightarrow \ \ & \lambda_t = \mu_\psi \tau \ \ + \ \ \mu_\psi \times \sum_{i = 1}^t \beta^{i - 1}(1 - \beta)\kappa X_{t - i} \ \ + \ \ \mu_\psi\beta^{t}(1 - \beta)\eta
\end{align*}
We can then re-write $\lambda_t$ as
\begin{align*}
\lambda_t & = \mu_\psi(1 - \beta)\tau + \mu_\psi(1 - \beta)\kappa X_{t - 1} + \beta \left[\mu_\psi\tau +   \mu_\psi \sum_{i = 2}^t \beta^{i - 1}(1 - \beta)\kappa X_{t - i}  \ \ + \ \ \mu_\psi\beta^{t - 1}(1 - \beta)\eta\right]\\
& = \underbrace{\mu_\psi(1 - \beta)\tau}_{\nu} \ + \ \underbrace{\mu_\psi(1 - \beta)\kappa}_{\alpha} X_{t - 1} \ + \ \beta \lambda_{t - 1}
\end{align*}
for $t \geq 2$. Combined with the relationship $X_t = \psi *_G (I_t + A_t) = \psi *_G N_t$ this is the form a CP-INGARCH(1, 1) model as introduced in \eqref{eq:N_CP_original}--\eqref{eq:lambda_CP_original}. Concerning the initialization of the process, the same argument as in Section \ref{subsec:poisson11} implies that we have to set $\lambda_0 = \mu_\psi\times \left\{\tau + (1 - \beta) \times \eta\right\}$.

% \newpage



\end{document}