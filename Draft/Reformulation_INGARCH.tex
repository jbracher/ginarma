\documentclass[review]{elsarticle}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{url}
\usepackage{graphicx}
\usepackage{todonotes}

\begin{document}

\title{A new thinning-based representation of the compound Poisson INGARCH(1, 1) model}
\author{Johannes Bracher}
\address{Chair of Statistics and Econometrics, Karlsruhe Institute of Technology, \\ % Bl\"ucherstra{\ss}e 17, 76185 Karlsruhe\\
Computational Statistics Group, Heidelberg Institute for Theoretical Studies}


\newcommand{\juv}{S}



\begin{abstract}
We propose a new thinning-based formulation of the Poisson and certain compound Poisson (CP) INGARCH(1, 1) models which allows for an intuitive interpretation and the application of branching processes theory to derive stochastic properties. Notably, it is straightforward to show that under mild conditions the CP-INGARCH(1, 1) model is geometrically ergodic while all moments of the limiting-stationary distribution are finite. Moreover we point out how a Markov chain approach can be used to approximate the limiting-stationary distribution with arbitrary precision.
\end{abstract}

\begin{keyword}
branching process, compound distribution, count time series, geometric ergodicity, integer-valued GARCH
\end{keyword}

\maketitle


\section{Poisson and compound Poisson INGARCH(1, 1) models}
\label{sec:original_formulation}

The Poisson INGARCH(1, 1) model \cite{Ferland2006, Fokianos2009} is one of the most widely used count time series models. It is defined as a process $\{X_t, t \in \mathbb{N}\}$ with
\begin{align}
X_t \mid X_{t - 1}, \dots, X_0, \lambda_0 & \sim \text{Pois}(\lambda_t)\label{eq:X_t_original}\\
\lambda_t & = \nu + \alpha X_{t - 1} + \beta \lambda_{t - 1}. \label{eq:lambda_t}
\end{align}
Here we assume $\nu > 0, \alpha \geq 0$ and $0 \leq \beta < 1$. Note that the latter is somewhat more restrictive than most definitions from the literature where $\beta \geq 1$ is usually allowed. The initial values $\lambda_0 \geq \nu$ and $X_0 \in \mathbb{N}_0$ are assumed to be fixed.

Numerous extensions of this model have been suggested, notably using conditional distributions other than the Poisson (see \cite{Weiss2018} for an overview). A broad class, discussed in \cite{Goncalves2015} and \cite{Silva2016}, are \textit{compound Poisson} INGARCH models, where the Poisson distribution in \eqref{eq:X_t_original} is replaced by a compound Poisson distribution. A random variable $Y$ is said to follow a compound Poisson distribution \cite[Chapter 3]{Feller1968} if it can be written as a randomly stopped sum 
$$
Y = \sum_{i = 1}^N Z_i \text{ with } Z_1, \dots, Z_N \stackrel{\text{iid}}{\sim} G(\psi)
$$
where $N$ follows a Poisson distribution. In the following we assume that $G(\psi)$, also called the \textit{secondary distribution}, is parameterized by a single parameter $\psi$ and has support $\mathbb{N}_0$. Its mean $\mu_\psi$ and variance $\sigma^2_\psi$ are assumed to be finite.

In this work we are thus concerned with processes $\{X_t, t \in \mathbb{N}\}$ of the form % N_{t - 1}, N_{t - 2}, \dots, N_0,
\begin{align}
N_t \ \mid \ X_{t - 1}, X_{t - 2}, \dots, X_0, \lambda_0 & \sim \text{Pois}(\lambda_t/\mu_\psi) \label{eq:N_CP_original}\\
X_t & = \sum_{i = 1}^{N_t} Z_{i, t} \ \ \text{where} \ \  Z_{i, t} \stackrel{\text{iid}}{\sim} G(\psi)\label{eq:X_CP_original}\\
\lambda_t & = \nu + \alpha X_{t - 1} + \beta \lambda_{t - 1},\label{eq:lambda_CP_original}
\end{align}
where $\lambda_0 > 0$ is fixed. Conditional on the past, $X_t$ is thus assumed to follow a compound Poisson distribution with mean $\lambda_t$. In \eqref{eq:N_CP_original}, the condition is also on $N_{t - 1}, \dots, N_1$, but we suppressed this to shorten notation. This formulation is somewhat more restrictive than the class discussed in \citep{Goncalves2015} where $\psi_t$ is allowed to depend on the past of the process, too. It nonetheless contains a number of well-known models \cite[Observation 2]{Goncalves2015}:

\begin{itemize}
\item When choosing $G(\psi)$ such that $\text{Pr}(Z_{it} = 1) = 1$ we recover the Poisson INGARCH(1, 1) model.
\item If we choose $G$ to be a logarithmic distribution % $\text{Log}(\psi)$ with mean $\mu_\psi = \psi/\{(1 - \psi)\log(1 - \psi)\}$, equations \eqref{eq:N_CP_original}--\eqref{eq:X_CP_original} imply
% $$
% X_t  \ \mid \ X_{t - 1}, X_{t - 2}, \dots, X_0, \lambda_0 \sim \text{NegBin}(r_t, p),
% $$
% where \todo{(re-check)}
% $$
% r_t = \frac{\lambda_t(1 - \psi)}{\psi}, \ \ p = 1 - \psi.
% $$
% This corresponds to
we obtain a generalization of the NB-DINARCH model \cite{Xu2012} to an INGARCH(1, 1) structure (see also Example 4.2.1 in \cite{Weiss2018}). %, see also Observation 2 in \cite{Goncalves2015} and Example 4.2.1 in \cite{Weiss2018} .
\item The Neyman Type A INGARCH model \cite{Goncalves2015a} results when setting $G$ to a Poisson distribution. %$\text{Pois}(\psi)$ distribution. The conditional distribution of $X_t$ given the past is then \todo{re-check parameterization of Neyman Type A}
% $$
% X_t  \ \mid \ X_{t - 1}, X_{t - 2}, \dots, X_0, \lambda_0 \sim \text{NeymanTypeA}(\lambda_t/\psi, \psi).
% $$
% \item Hermite?
\item The generalized Poisson (GP) INGARCH(1, 1) model \cite{Zhu2012} is obtained by setting $G(\psi)$ to a Borel distribution. % $\text{Borel}(\psi)$. The conditional distribution of $X_t$ is then given by
% $$
% X_t  \ \mid \ X_{t - 1}, X_{t - 2}, \dots, X_0, \lambda_0 \sim \text{GP}(\lambda_t [1 - \psi], \psi).
% $$
\end{itemize}
% The negative binomial INGARCH model by \cite{Zhu2011} is \textit{not} part of the class as it would require the parameter $\psi_t$ of the secondary distribution to depend on $N_t$ .

\section{A thinning-based representation of the CP-INGARCH(1, 1) model}
\label{sec:alternative_formulation}

We provide an alternative representation of the CP-INGARCH(1, 1) process \eqref{eq:N_CP_original}--\eqref{eq:lambda_CP_original}. It involves exlusively discrete-valued processes and is given by
\begin{align}
X_t & = \psi *_G (I_t + H_t),\label{eq:X_t}\\
\juv_t & = R_{t - 1} + L_{t - 1}\label{eq:juv_t}
\end{align}
where
\begin{align}
H_t & = (1 - \beta) \circ S_t\\
R_t & = S_t - H_t \label{eq:R_t}\\
L_t & = \kappa \star X_t \label{eq:L_t}
% H_t & = \psi *_G (S_t - R_t)
\end{align}
and $\kappa \geq 0, 0 \leq \beta <1$. Here, the sequence $I_t$ consists of independent Poisson random variables with rate $\tau > 0$ while $\circ$ and $\star$ denote binomial and Poisson thinning, respectively, i.e.\
\begin{align*}
\pi \circ N & = \sum_{i = 1}^N Z_i \ \ \ \text{with} \ \ \ Z_i \stackrel{\text{ind}}{\sim} \text{Bin}(1, \pi), \\
\pi \star N & = \sum_{i = 1}^N Z_i \ \ \ \text{with} \ \ \ Z_i \stackrel{\text{ind}}{\sim} \text{Pois}(\pi).
\end{align*}
The remaining operator $*_G$ is defined as
$$
\psi *_G N = \sum_{i = 1}^N Z_i \ \ \ \text{ with } \ \ \ Z_i \stackrel{\text{ind}}{\sim} G(\psi)
$$
and thus denotes summing over independent samples from the distribution $G(\psi)$. To initialize the process we fix $X_0 \in \mathbb{N}_0, \eta > 0$ and specify $R_0 \sim \text{Pois}\left(\eta \right)$.

It can be shown that $\{X_t\}$ as defined in \eqref{eq:X_t}--\eqref{eq:L_t} is a CP-INGARCH(1, 1) process as given in \eqref{eq:X_t_original}--\eqref{eq:lambda_t}, see \ref{appendix:proof} for the derivation. The parameters $\beta$ and $\psi$ as well as the type of the secondary distribution $G$ are shared across the two formulations. The remaining parameters $\nu, \alpha$ and $\lambda_0$ of the original formulation \eqref{eq:X_t_original}--\eqref{eq:lambda_t} can be recovered as
\begin{align*}
\nu & = \mu_\psi \tau(1 - \beta); \ \
\alpha = \mu_\psi \kappa(1 - \beta); \ \
\lambda_0 = \mu_\psi \left(\tau + \frac{1 - \beta}{\beta} \times \eta\right).
\end{align*}
Here, $\mu_\psi$ is again the mean of the distribution $G(\psi)$. \todo{check for $\lambda_0$}

The formulation \eqref{eq:X_t}--\eqref{eq:L_t} may seem a little involved at first sight, but allows for an intuitive interpretation, visualized in Figure \ref{fig:ingarch_flowchart}. We frame it in terms of clusters of eggs and resulting offspring, a common interpretation of compound count distributions \cite{Neyman1939}.

\begin{itemize}
\item $X_t$ is the number of females in an animal population at (discrete) time $t$. Each of these females only stays alive for one time period and produces
 a $\text{Pois}(\kappa)$-distributed number of egg clusters. The total number of such clusters \textit{laid} at time $t$ is denoted by $L_t = \kappa \star X_t$.
\item Clusters laid at time $t$ enter into a \textit{stock} of clusters at time $t + 1$, and the total number of clusters in stock at time $t + 1$ is denoted by $\juv_{t + 1}$. At each time $t$, each of the $S_t$ available clusters independently has a probability of $1 - \beta$ that new animals \textit{hatch} from all contained eggs simulataneously; the number of clusters for which this is the case is denoted by $H_t = (1 - \beta) \circ S_t$. Otherwise, i.e.\ with probability $\beta$, clusters remain in the stock and form part of $\juv_{t + 1}$. The number of these \textit{remaining} clusters is denoted by $R_t = S_t - H_t$.
\item For each of the $H_t$ hatching clusters the number of new females independently follows the distribution $G(\psi)$. These new females enter into $X_t$.
\item At each time $t$, new females hatched from a $\text{Pois}(\tau)$ distributed number $I_t$ of clusters \textit{immigrate} into the population. The number of new females from each cluster again independently follows a $G(\psi)$ distribution.
\end{itemize}

\begin{figure}[h!]
\includegraphics[scale = 0.8]{figure/flowchart_ingarch.pdf}
\caption{Visualization of the INGARCH(1, 1) formulation \eqref{eq:X_t}--\eqref{eq:R_t}  as a flow chart. Solid lines represent results of binomial thinning, dashed lines Poisson thinning, double lines thinning with $*_G$.}
\label{fig:ingarch_flowchart}
\end{figure}

% Thinning-based formulations of INGARCH(1, 1) models also appear in \cite{Ferland2006} and \cite{Goncalves2015}. However, these take the form of successive approximations approaching the INGARCH(1, 1) in the limit and thus provide a less direct handle on stochastic properties of the process. 

Note that a related model where the thinning step with $*_G$ is omitted and equation \eqref{eq:L_t} is replaced by
$L_t = \kappa \circ X_t$ has been introduced in \cite{Bracher2019}.

\section{Derivation of some stochastic properties}

As will be shown in the following, the novel representation of CP-INGARCH(1, 1) models establishes a link to branching process theory. This makes it a useful tool to derive stochastic properties of these models in a straightforward manner.

\subsection{Geometric ergodicity}

The Poisson INGARCH(1, 1) model has been shown to be geometrically ergodic by Fokianos et al \cite{Fokianos2009}, who considered an approximation by a sequence of perturbed processes. Ergodicity of various generalizations has been adressed by several authors \citep{Davis2016, Douc2013, Neumann2011}. Goncalves et al \citep{Goncalves2015} cover ergodicity (but not geometric ergodicity) of CP-INGARCH models. A proof of geometric ergodicity of the GP-INGARCH(1, 1) model has been sketched by Zhu \citep{Zhu2012}. %  Other proofs of (geometric) ergodicity of INGARCH models have been given in the following works. Neumann \citep{Neumann2011} provides a contractive condition for ergodicity of a more general model class which includes non-linear autoregressive models, but also the Poisson INGARCH(1, 1). Zhu \citep{Zhu2012} states that the same technique can also be used to establish geomtric ergodicity of the GP-INGARCH(1, 1) model. Ergodicity of broader classes which are not limited to a conditional Poisson or generalized Poisson distribution have been considered by Douc et al \citep{Douc2013} and Davis and Liu \cite{Davis2016}. Goncalves et al \cite{Goncalves2015} established ergodicity (but not geometric ergodicity) of CP-INGARCH models. %, including higher-order models. % Multivariate Poisson INGARCH models have been analyzed by Fokianos et al \cite{Fokianos2020}.% However, to our best knowledge, geometric ergodicity has so far only been established for the Poisson INGARCH(1,1) model.\todo{Add Fokianos multivariate}

All of the mentioned proofs can be described as technically involved. As noted by Neumann \cite{Neumann2011}, the main difficulty lies in the fact that the ``innovations'' $X_t$ in \eqref{eq:X_t_original} are integer-valued, while the condtional mean process $\{\lambda_t\}$ from \eqref{eq:lambda_t} is real-valued. The re-formulation \eqref{eq:juv_t}--\eqref{eq:R_t} involves exclusively integer-valued processes, and allows us to exploit results from branching process theory.

Plugging equations \eqref{eq:X_t} and \eqref{eq:L_t} into \eqref{eq:juv_t}, $\{\juv_t\}$ can be written as
\begin{align}
\juv_t = R_{t - 1} + \kappa \star (\psi *_G H_{t - 1}) + \kappa \star (\psi *_G I_{t - 1}). \label{eq:recursion_S}
\end{align}
Here, $\kappa \star (\psi *_G I_{t - 1})$ represents clusters laid by immigrated females. The term  $R_{t - 1} = \beta \circ \juv_{t - 1}$ contains clusters remaining from $ \juv_{t - 1}$, while $\kappa \star (\psi *_G H_{t - 1}) = \kappa \star \{\psi *_G (S_{t - 1} - R_{t - 1})\}$ are new clusters laid by ``native'' (i.e., not immigrated) females at time $t$. Using the same decomposition, we note that each cluster from $\juv_{t - 1}$ can either remain in the stock itself or hatch and generate a $G(\psi)$ distributed number of new females, which in turn each add a $\text{Pois}(\kappa)$ distributed number of clusters to the stock. We can thus write recursion \eqref{eq:recursion_S} as
$$
\juv_t = \sum_{i = 1}^{\juv_{t - 1}} A_{t, i} \ \ + \ \ I^*_t.
$$
Here, $A_{t, i}$ is the number of clusters the $i$-th cluster from $S_t$ contributes to $S_{t + 1}$ and $I^*_t$ are clusters contributed by the immigrants $I_{t - 1}$:
\begin{align}
A_{t, i} & = \begin{cases}
1 & \text{with probability } \beta\\
\kappa \star (\psi *_G 1) & \text{with probability } 1 - \beta, \text{independently for each } i
\label{eq:Z_t_i}
\end{cases}\\
I^*_t & = \kappa\star (\psi *_G I_{t - 1}). \label{eq:I_star}
\end{align}
Thus, $\{\juv_t\}$ is a Galton-Watson branching process where the offspring distribution \eqref{eq:Z_t_i} is a specific one-inflated compound distribution. %, specifically, $A_{t, i}$ is a $G(\psi)$-stopped sum of Poisson random variables. 
The immigration distribution \eqref{eq:I_star} is a compound Poisson distribution where the secondary distribution is itself a Poisson mixture. % This follows from the fact that given $I_t$,  %are generated by two compounding steps. % and thus follow a Poisson Poisson compound or Neyman type A distribution with parameters $\kappa$ and $\tau$ \cite{Masse2005}.

Theory on branching processes with immigration, specifically Theorem 1 from Pakes \cite{Pakes1971} tells us that $\{\juv_t\}$ is geometrically ergodic if (a) $\mathbb{E}(A_{i, t}) < 1$, (b) $\mathbb{E}[(A_{i, t} + 1)\log(A_{i, t} + 1)] < \infty$, (c) $\mathbb{E}(I^*_t) < \infty$. These conditions are easily verified for $\{S_t\}$ provided that $\kappa\mu_\psi < 1$ and, as previously assumed, $\mu_I, \sigma^2_I < \infty$.
%\begin{itemize}
%\item[(a)] $\mathbb{E}(A_{i, t}) < 1$,
%\item[(b)] $\mathbb{E}[(A_{i, t} + 1)\log(A_{i, t} + 1)] < \infty$,
%\item[(c)] $\mathbb{E}(I^*_t) < \infty$.
%\end{itemize}
% Condition (a) holds if $\kappa\mu_\psi < 1$. For condition (b) note that $\mathbb{E}[(A_{i, t} + 1)\log(A_{i, t} + 1)] < \mathbb{E}(A_{i, t}^2)$. As $A_{i, t}$ comes from a mixture of two distributions which both have finite second moments (noting that $\text{Var}[\kappa \star (\psi *_G 1)] = \kappa(\mu_\psi + \sigma^2_\psi) < \infty$ \todo{re-check!}) we have $\mathbb{E}(A_{i, t}^2) < \infty$. Condition (c) holds as $\mathbb{E}(I^*_t) = \kappa\mu_\psi\tau < \infty$.

% (For completeness we note that $\text{Var}[\kappa \star (\psi *_G 1)] = \mathbb{E}[\text{Var}\{\kappa \star (\psi *_G 1)\} \ \mid \ \psi *_G 1] + \text{Var}[\mathbb{E}\{\kappa \star (\psi *_G 1)\}  \ \mid \ \psi *_G 1] = \kappa\mu_\psi $)

Like in Fokianos et al \cite{Fokianos2009}, Proposition 1 from Meitz and Saikkonen \cite{Meitz2008} can then be used to show that geometric ergodicity is also inherited by the joint process $\{(\juv_t, R_t, H_t, X_t, L_t)\}$. %, see Appendix \ref{appendix:proof_meitz}.
Even though it is in principle sufficient to initialize the process with $R_0$ and $L_0$ as in Section \ref{sec:alternative_formulation}, we now assume that the process $\{(\juv_t, R_t, H_t, X_t, L_t)\}$ is initialized by a vector $(s_0, r_0, h_0, x_0, l_0)$. Geometric ergodicity of $\{(\juv_t, R_t, H_t, X_t, L_t)\}$ is then established by verifying the two following conditions (Assumption 1 in \cite{Meitz2008}):

\begin{enumerate}
\item Given $(\juv_u, R_u, H_u, X_u, L_u), 0 \leq u < t$ and $\juv_t$, $(R_t, H_t, X_t, L_t)$ depends only on $\juv_t$. It is straightforward to see from the model definition \eqref{eq:juv_t}--\eqref{eq:L_t} or Figure \ref{fig:ingarch_flowchart} that this is fulfilled.
\item There is an $n \geq 1$ such that for all $t > n$, the generation mechanism of $\juv_t \ \mid \ \juv_0 = s_0, R_0 = r_0, H_0 = h_0, X_0 = x_0, L_0 = l_0$ has the same structure as that of $\juv_t \ \mid \ \juv_n = \tilde{s}_n$, where $\tilde{s}_n$ is some function of $(s_0, r_0, h_0, x_0, l_0)$. As $(s_0, r_0, h_0, x_0, l_0)$ only impacts the further course of the process $\{\juv_t\}$ through $\juv_1 = l_0 + r_0$, this is the case for $n = 1$ and $\tilde{s}_1 = l_0 + r_0$.
\end{enumerate}
% This proves geometric ergodicity of the joint process $\{(\juv_t, R_t, X_t, L_t\}$ if $\kappa\mu_\psi < 1$ (and $\sigma_\psi^2 < \infty$, as assumed since Section \ref{sec:original_formulation}).

\subsection{Existence of higher moments}

Existence of higher-order moments of CP-INGARCH(1, 1) models with time-constant $\psi$ has been proven in \cite{Silva2016}, but the proof is again quite involved. Representation \eqref{eq:X_t}--\eqref{eq:L_t} allows for a more condensed argument.

It is known that if the offspring and immigration distributions of a subcritical Galton-Watson branching process have finite $r$-th moments, this is also the case for the limiting-stationary distribution \cite[Sec. 4]{Lange1981}. Both conditions are fulfilled for $\{S_t\}$ if the $r$-th moment of $G(\psi)$ is finite and $\kappa\mu_G < 1$:

\begin{itemize}
\item A randomly stopped sum $Y = \sum_{i = 1}^N Z_i$ of i.i.d. random variables $Z_i$ has finite $r$-th moments if both $N$ and the $Z_i$ have finite $r$-th moments \cite[Theorem 5.2]{Gut2009}. The Poisson distribution has finite moments of any order. Thus, if the $r$-th moment of $G(\psi)$ is finite, this also holds for the offspring distribution $\kappa \star (\psi *_G 1)$ from \eqref{eq:Z_t_i}.
\item Similar arguments imply that if $G(\psi)$ has a finite $r$-th moment, this is also the case for $\psi *_G I_{t - 1}$ and in a second step the immigration process $I^*_t = \kappa \star(\psi *_G I_{t - 1})$ from equation \eqref{eq:I_star}.
\end{itemize}
Consequently, the limiting-stationary distribution of $\{S_t\}$ has finite moments up to order $r$ if this is the case for the secondary distribution $G(\psi)$ (while $\kappa\mu_G < 1$). The same arguments imply that finiteness of moments translates to $H_t = (1 - \beta) \circ S_t$, $\psi *_G H_t$ and ultimately $X_t = \psi *_G I_t + \psi *_G H_t$.

\subsection{Approximating the limiting-stationary distribution}

As $\{X_t\}$ is ergodic, the limits
$$
p_i = \lim_{t \rightarrow \infty} \text{Pr}(X_t = i \ \mid R_0 = r_0, L_0 = l_0)
$$
exist and are independent of the initialization of the process. However, even in the Poisson case no closed form for the $p_i$ is known. For INARCH(1) models, i.e. the special case $\beta = 0$ of the INGARCH(1, 1), a Markov chain approach can serve to approximate the $p_i$ with arbitrary precision \cite{Weiss2010}. This is based on the fact that the INARCH(1) is a discrete first-order Markov chain so that
$$
\text{Pr}(X_t = i) = \sum_{j = 0}^\infty p_{j|i} \times \text{Pr}(X_{t - 1} = j).
$$
Here, $p_{j|i} = \text{Pr}(X_t = i \ | \ X_{t - 1} = j) $ is the transition probability from $j$ to $i$. Choosing $M$ and $T$ sufficiently large, the approximation is given by
$$
p_i \approx \text{Pr}(X_T = i) \ \ \text{where} \ \ \text{Pr}(X_t = i) \approx \sum_{j = 0}^M p_{j|i} \times \text{Pr}(X_{t - 1} = j).
$$

This method, however, is not directly applicable to the INGARCH(1, 1) model \eqref{eq:N_CP_original}--\eqref{eq:lambda_CP_original} as $\{X_t\}$ is not a first-order Markov chain. Application to the joint process $\{(X_T, \lambda_t)\}$, which is a first-order Markov chain, is not feasible as $\lambda_t$ has a continuous support. Formulation \eqref{eq:X_t}--\eqref{eq:L_t}, however, enables application of the Markov chain approach. $\{S_t\}$ is a discrete first-order Markov chain where
$$
p^{(S)}_i \approx \text{Pr}(S_T = i) \ \ \text{where} \ \ \text{Pr}(S_t = i) \approx \sum_{j = 0}^M p^{(S)}_{j|i} \times \text{Pr}(S_{t - 1} = j)
$$
and subsequently
$$
p_i = \sum_{k = 0}^M p_k^{(S)} \times \text{Pr}(X_t = i \ \mid \ S_t = k).
$$
% While it is somewhat tedious to compute the transition probabilities $p^{(S)}_{j|i}$ of $\{S_t\}$ as well as the probabilities $\text{Pr}(X_t = i \ \mid \ S_t = k)$ linking $\{S_t\}$ and $\{X_t\}$, this approach allows to evaluate the $p_t$ with arbitrary precision.


%Show ergodicity of S via branching process with immigration.
%
%\textbf{Link to Weiss' paper with bridge between inar and inarch? $\juv_t$ No.}
%
%Geometric ergodicity of the process $(\lambda_t, X_t)$ of the INGARCH(1, 1) model as defined in \eqref{eq:X_t}--\eqref{eq:R_t} has been demonstrated by several authors. Fokianos et al \cite{Fokianos2009} use a 
%
%The joint process $(R_t, X_t)$ is a discrete first-order Markov chain with support $\mathbb{N}_0^2$. As $X_{t + 1} + R_{t + 1} \geq R_{t}$, not all states of the Markov chain can be reached from all other states in just one step. However, this is possible within two steps. E.g., a possibility to move from $(m_t, x_t)$ at time $t$ to $(m_{t + 2}, x_{t + 1})$ in two steps is the sequence of events
%$$
%R_t = \juv_t;\ \
%I_{t + 1} = m_{t + 2} + x_{t + 2}
%$$
%implying
%$$
%X_{t + 1} = m_{t + 2} + x_{t + 1};\ \ \juv_{t + 1} = 0;\ \ R_{t + 1} = 0
%$$
%followed by
%\begin{align*}
%L_{t + 1} = m_{t + 2} + x_{t + 1}.
%\end{align*}
%This in turn implies
%$$
%\juv_{t + 2} = \juv_{t + 2} + x_{t + 1}
%$$
%which allows us to also reach
%$$
%R_{t + 2} = m_{t + 2};\ \
%X_{t + 2} = x_{t + 2}
%$$
%As this sequence of events has a non-zero probability, the Doeblin condition \cite{Zubkov2011} is fulfilled and implies geometric ergodicity of the Markov chain $(R_t, X_t)$.
%
%% Existence of three derivatives of the log likelihood function is shown by Fokianos et al. Then Jensen and Rahbek's law of large numbers for geometrically ergodic processes implies normality of the maximum likelihood estimators.

% \section{Discussion}

% We have provided an alternative representation of the compound \todo{to be extended} Poisson INGARCH(1, 1) model and used it to obtain several properties of the process. Notably, the fact that the process is geometrically ergodic could be useful for studies of the behaviour of conditional maximum likelihood estimators along the lines of \cite{Fokianos2009} and \cite{Zhu2012}. % This question has not yet been addressed as existing studies are limited to conditional least squares (CLS) estimators \cite{Silva2016} and Poisson quasi-maximum likelihood estimators \cite{AhmadXXX}.

% Mention that CLS estimator is normal according to Cardoso da Silva. PQML is normal according to Ahmad and Francq.

\appendix
\section{Equivalence of the two INGARCH(1, 1) formulations}
\label{appendix:proof}

We demonstrate that the process $\{X_t, t = 1, 2, \dots\}$ from \eqref{eq:juv_t}--\eqref{eq:R_t} is equivalent to the INGARCH(1, 1) process \eqref{eq:X_t_original}--\eqref{eq:lambda_t}. We start by decomposing the terms $L_t, t = 0, 1, \dots$ and $R_0$ by when the respective clusters will hatch. We denote by $L_t^{(i)}$ the number of clusters laid at time $t$ and hatching at $t + i$ and by $R^{(i)}_0$ the number of clusters initially in the stock and hatching at time $i$. This implies
\begin{align*}
L_t & = \sum_{i = 1}^\infty L_t^{(i)}, \ \ \ 
R_0 = \sum_{i = 1}^\infty R_0^{(i)}, \ \ \ 
H_t = \sum_{i = 1}^{t} L_{t - i}^{(i)} \ \ + \ \ R_0^{(t)}.
% R_t & = \sum_{i = 1}^t \sum_{j > i} L_{t - i}^{(j)} \ \ + \ \ \sum_{j > t} R_0^{(j)}.
\end{align*}
A cluster laid at time $t$ has a probability of $\beta^{i - 1}(1 - \beta)$ to hatch at time $t + i, i = 1, 2, \dots$, and thus be part of $L_t^{(i)}$. The Poisson splitting property \cite{Kingman1993} then implies that given $X_t$, the $L_t^{(i)}$ are independently Poisson distributed,
$$
L_t^{(i)} \mid X_t, X_{t - 1}, \dots, X_0 \stackrel{\text{ind}}{\sim} \text{Pois}(\beta^{i - 1}[1 - \beta]\kappa X_t). % ; \ \ L_t^{(i)} \perp L_t^{(j)} \mid X_t, i \neq j.
$$
Given $X_t$, $L_t^{(i)}$ does not have any impact on the further course of the process $\{X_t\}$ until time $t + i$, so that for $t < u < t + i$ we moreover have $L_t^{(i)} \perp L_u^{(j)} \mid X_t, X_u$. We can now re-write
\begin{align}
X_t & = \psi *_G I_t \ \ + \ \ \psi *_G\left(\sum_{i = 1}^{t} L_{t - i}^{(i)} \ \ + \ \ R_0^{(t)}\right)\nonumber \\
& = \psi *_G \underbrace{\left( I_t \ \ + \ \ \sum_{i = 1}^{t} L_{t - i}^{(i)} \ \ + \ \ R_0^{(t)}\right)}_{\text{denote this by } N_t}. \label{eq:introduce_N_T}
\end{align}
Moreover we note that in analogy to the above argument, $R_0^{(t)}$ is Poisson distributed with rate $\eta\beta^{t - 1}(1 - \beta)$ and independent of all $X_t$ and $L_t^{(i)}$ up to $t - 1$. Conditioned on $X_{t - 1}, \dots, X_0, \eta$, we thus have that the term $N_t$ introduced in equation \eqref{eq:introduce_N_T} is a sum of independent Poisson random variables. This implies
$$
N_t \mid X_{t - 1}, \dots, X_0, \eta \sim \text{Pois}(\lambda_t)
$$
with
$$
\lambda_t = \tau \ \ + \ \ \sum_{i = 1}^t \beta^{i - 1}(1 - \beta)\kappa X_{t - i} \ \ + \ \ \beta^{t - 1}(1 - \beta)\eta.
$$
The conditional expectation $\lambda_t$ can be re-written as
\begin{align*}
\lambda_t & = \tau(1 - \beta) + (1 - \beta)\kappa X_{t - 1} + \beta \left[\tau +   \sum_{i = 2}^t \beta^{i - 1}(1 - \beta)\kappa X_{t - i}  \ \ + \ \ \beta^{t - 2}(1 - \beta)\eta\right]\\
& = \tau(1 - \beta) + (1 - \beta)\kappa X_{t - 1} + \beta \lambda_{t - 1}.
\end{align*}
Combined with the relationship $X_t = \psi *_G N_t$ from equation \eqref{eq:introduce_N_T} this is the form a CP-INGARCH(1, 1) model as introduced in \eqref{eq:N_CP_original}--\eqref{eq:lambda_CP_original}. We conclude by considering the initialization of the process, where we have
$$
\lambda_1 = \tau(1 - \beta) + (1 - \beta)\kappa X_0 + \beta\left(\tau + \frac{1 - \beta}{\beta} \times \eta \right),
$$
meaning that we have to set $\lambda_0 = \tau + (1 - \beta)/\beta \times \eta$.

%\section{Proof of geometric ergodicity of $\{\juv_t\}$ following Pakes}
%\label{appendix:proof_pakes}
%
%Theorem 1 of Pakes \cite{Pakes1971} tells us that $\{\juv_t\}$ is geometrically ergodic if $\kappa < 1$ and
%
%\begin{align*}
%\mathbb{E}[(Z_{i, t} + 1)\log(Z_{i, t} + 1)] & < \infty\\
%\mathbb{E}(I^*_t) < \infty
%\end{align*}
%
%As $\mathbb{E}(Y_t) = \kappa\tau$. the first condition obviously holds. For the second condition note that $\mathbb{E}[(Z_{i, t} + 1)\log(Z_{i, t} + 1)] < \mathbb{E}(Z_{i, t}^2) < \infty$ as $Z_{i, t}$ comes from a mixture of two distributions which both have finite variance. 

% \section{Proof of geometric ergodicity of $\{(\juv_t, R_t, X_t, L_t)\}$ following Meitz and Saikkonen}
% \label{appendix:proof_meitz}

% \textbf{MAybe set $R_0 = 0$ to avoid some technical difficulties?}

%Even though it is in principle sufficient to choose initial distributions for $R_0$ and $L_0$ as in Section \ref{sec:alternative_formulation}, we now assume that the process $\{(\juv_t, R_r, X_t, L_t)\}$ is initialized by a vector $(\juv_0, r_0, x_0, L_0)$. Proposition 1 of Meitz and Saikkonen \citep{Meitz2008} can then be employed to show that the geometric ergodicity of $\{\juv_t\}$ is inherited by $\{(\juv_t, R_t, X_t, L_t)\}$. The two following conditions need to be met (Assumption 1 in \cite{Meitz2008}):
%
%\begin{enumerate}
%\item Given $(\juv_u, R_u, X_u, L_u), 0 \leq u < t$ and $\juv_t$, $(R_t, X_t, L_t)$ depends only on $\juv_t$. It is straightforward to see from the model definition \eqref{eq:juv_t}--\eqref{eq:L_t} or Figure \ref{fig:ingarch_flowchart} that this is fulfilled.
%\item There is an $n > 1$ such that for all $t > n$, the generation mechanism of $\juv_t \ \mid \ (\juv_0, R_0, X_0, L_0, t) = (\juv_0, r_0, x_0, L_0)$ has the same structure as that of $\juv_t \ \mid \ \juv_n = \tilde{s}_n$, where $\tilde{s}_n$ is some function of $(\juv_0, r_0, x_0, L_0)$. As $(\juv_0, r_0, x_0, L_0)$ only impacts the further course of the process $\{\juv_t\}$ through $\juv_1 = L_0 + m_0$, this is the case for $n = 1$ and $\tilde{s}_1 = L_0 + m_0$.
%\end{enumerate}


\bibliographystyle{plain}
\bibliography{bib_ingarch.bib}

\end{document}